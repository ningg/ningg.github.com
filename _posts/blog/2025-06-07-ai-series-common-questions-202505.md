---
layout: post
title: LLM 常见 50 题.
description: 常见题目的总结，作为引子，辅助理解 LLM
published: true
category: AI
---

> 原文： [https://weibo.com/1233486457/Pvl0V8RZy](https://weibo.com/1233486457/Pvl0V8RZy)


精选50个LLM核心面试问题，涵盖关键概念、技术和挑战。
作者：Hao Hoang2025年5月
PDF地址：drive.google.com/file/d/1wolNOcHzi7-sKhj5Hdh9awC9Z9dWuWMC/view

1. 什么是分词？
将文本分解为令牌，如”artificial”→”art”+“ific”+“ial”。LLM处理令牌数值表示而非原始文本，支持多语言处理和未知词处理。

2. 注意力机制如何工作？
通过查询(Q)、键(K)、值(V)向量计算相似度权重，聚焦相关令牌。在”猫追老鼠”中帮助连接”老鼠”与”追”。

3. 什么是上下文窗口？
LLM一次处理的令牌数量。大窗口(32K令牌)提升连贯性但增加计算成本，需平衡效率与性能。

4. LoRA与QLoRA区别？
LoRA通过低秩矩阵实现高效微调；QLoRA在此基础上应用4位量化进一步减少内存，可在单GPU微调70B模型。

5. 束搜索vs贪婪解码？
贪婪解码选择最可能词；束搜索保持k个候选序列，平衡概率与多样性，产生更连贯输出。

6. 温度参数作用？
控制生成随机性。低温度(0.3)产生保守输出，高温度(1.5)增加创造性，0.8平衡创造性与连贯性。

7. 掩码语言建模？
隐藏随机令牌训练模型预测，BERT使用MLM实现双向理解，为下游任务建立语义基础。

8. 序列到序列模型？
编码器处理输入，解码器生成输出。应用于机器翻译、文本摘要等可变长度输入输出任务。

9. 自回归vs掩码模型？
自回归(GPT)顺序预测擅长生成；掩码(BERT)双向预测适合理解。训练目标决定优势方向。

10. 什么是嵌入？
令牌在连续空间的密集向量表示，捕获语义和句法属性。随机初始化或预训练后在任务中微调。

11. 下一句预测？
训练模型判断句子连续性，BERT学习50%正向/50%负向句子对分类，提升对话和摘要连贯性。

12. Top-k vs Top-p采样？
Top-k从k个最可能令牌采样；Top-p从累积概率达p的令牌采样，更灵活适应不同上下文。

13. 提示工程重要性？
设计输入引出期望响应。清晰提示如”100字总结”比模糊指令效果好，在零样本/少样本中特别有效。

14. 如何避免灾难性遗忘？
重放(混合新旧数据)、弹性权重合并(保护关键权重)、模块化架构(添加任务模块)。

15. 模型蒸馏？
训练小”学生”模型模仿大”教师”模型输出，使用软概率降低计算需求，支持移动设备部署。

16. 处理OOV词汇？
使用BPE等子词分词将未知词分解为已知单元，如”cryptocurrency”→”crypto”+“currency”。

17. Transformer vs Seq2Seq？
并行处理(非顺序RNN)、长程依赖捕获、位置编码保持序列顺序，显著提升可扩展性。

18. 过拟合缓解？
正则化(L1/L2惩罚)、Dropout(随机禁用神经元)、早停(验证性能平稳时停止)。

19. 生成式vs判别式？
生成式(GPT)建模联合概率创建新数据；判别式(BERT分类)建模条件概率区分类别。

20. GPT-4 vs GPT-3？
多模态输入、更大上下文(25K vs 4K令牌)、增强准确性减少事实错误。

21. 位置编码？
为Transformer添加序列顺序信息，使用正弦函数或学习向量确保位置相关的正确解释。

22. 多头注意力？
将Q、K、V分割到多个子空间，同时关注语法、语义等不同方面，提升复杂模式捕获。

23. Softmax在注意力中应用？
标准化注意力分数为概率分布：softmax(xi) = e^xi / Σj e^xj，确保关注重要令牌。

24. 点积在自注意力中作用？
计算Q·K相似度：Score = Q·K/√dk，高分数表示相关性，但O(n²)复杂度促使稀疏研究。

25. 交叉熵损失？
衡量预测与真实概率分歧：L = -Σyi log(ŷi)，优化模型为正确令牌分配高概率。

26. 嵌入梯度计算？
通过链式法则：∂L/∂E = ∂L/∂logits · ∂logits/∂E，调整向量表示优化性能。

27. 雅可比矩阵作用？
捕获输出对输入偏导数，在Transformer中计算多维梯度，确保准确参数更新。

28. 特征值与降维？
特征向量定义主方向，特征值表示方差。PCA选择高特征值向量降维同时保留主要信息。

29. KL散度？
量化分布差异：DKL(P||Q) = ΣP(x)log P(x)/Q(x)，评估模型预测与真实分布匹配度。

30. ReLU导数？
f’(x) = {1 if x>0; 0 otherwise}，稀疏性和非线性防止梯度消失，计算高效。

31. 链式法则在梯度下降中应用？
计算复合函数导数：d/dx f(g(x)) = f’(g(x))·g’(x)，实现逐层梯度计算和参数更新。

32. 注意力分数计算？
Attention(Q,K,V) = softmax(QK^T/√dk)V，缩放点积测量相关性，softmax标准化权重。

33. Gemini优化多模态训练？
统一架构(文本+图像处理)、高级注意力(跨模态稳定性)、数据效率(自监督减少标注需求)。

34. 基础模型类型？
语言模型(BERT、GPT)、视觉模型(ResNet)、生成模型(DALL-E)、多模态模型(CLIP)。

35. PEFT如何缓解遗忘？
只更新少量参数，冻结其余权重保留预训练知识。LoRA等技术确保适应新任务不失核心能力。

36. RAG步骤？
检索(获取相关文档)→排序(按相关性)→生成(基于检索内容响应)，提升事实准确性。

37. 专家混合(MoE)？
门控函数激活特定专家子网络，每次查询仅用10%参数，实现高效大规模计算。

38. 思维链(CoT)提示？
引导逐步推理，将复杂问题分解为逻辑步骤，提升数学推理和复杂任务准确性。

39. 判别式vs生成式AI？
判别式预测标签建模条件概率；生成式创建新数据建模联合概率，前者专于分类后者擅长创作。

40. 知识图谱集成？
提供结构化事实数据：减少幻觉(验证事实)、改善推理(实体关系)、增强上下文。

41. 零样本学习？
无任务特定数据直接推理，如”分类评论情感”，展示预训练知识的通用性。

42. 自适应Softmax？
按词频分组减少稀有词计算，降低大词汇表成本，加速训练推理，适合资源受限环境。

43. Transformer解决梯度消失？
自注意力(避免顺序依赖)、残差连接(直接梯度流)、层归一化(稳定更新)。

44. 少样本学习？
用最少例子执行任务，利用预训练知识。减少数据需求、快速适应、成本效益。

45. 修复偏见输出？
分析偏见源→使用平衡数据集和去偏技术→对抗训练或策划数据微调。

46. 编码器vs解码器？
编码器处理输入为抽象表示；解码器生成输出。翻译中编码器理解源语言，解码器产生目标语言。

47. LLM vs传统统计模型？
Transformer架构、大规模数据、无监督预训练 vs 简单监督方法(N-gram)。处理长程依赖和上下文嵌入。

48. 超参数重要性？
学习率、批量大小等控制训练。高学习率可能不稳定，需调参优化效率和准确性。

49. LLM定义？
在大量文本训练的AI系统，数十亿参数，擅长翻译、摘要、问答，利用上下文学习广泛适用。

50. 部署挑战？
资源密集(高计算需求)、偏见











[NingG]:    http://ningg.github.io  "NingG"










