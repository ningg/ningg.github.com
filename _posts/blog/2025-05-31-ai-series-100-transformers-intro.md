---
layout: post
title: AI百面：Transformer 原理 & 要点
description: Transformer 原理，引入一批典型术语、关键技术
published: true
category: AI
---

## 0.概要

> 在 Trae 下编辑文档，整理 Transformer 核心原理。



## 1.Transformer 原理


几个离散的知识点：

1. 词嵌入：Word Embedding，将词转换为「向量」，依赖分词算法？
2. 位置编码：Positional Encoding，普通的词向量，并无法表示出「词之间顺序信息」，因此，将词的顺序编码，也形成一个「位置向量」。
3. 矩阵：高维的向量运算，引入矩阵。



### 1.1.嵌入 Embedding

在机器学习和自然语言处理（NLP）中，嵌入（Embedding） 是一种将离散型数据（如词语、类别、符号）转换为连续型向量表示的技术。

其本质是，构建一个从`高维稀疏空间`到`低维稠密空间`的**映射**，旨在捕捉数据背后的语义或结构关系。



## 2.自注意力机制（Self-Attention Mechanism）

自注意力机制是Transformer的核心机制之一。它的基本思想是让模型在处理每个词汇时，都能够关注到输入序列中的所有其他词汇。这样，模型就能够根据整个输入序列的信息来生成每个词汇的表示。

在自注意力机制中，每个词汇的向量表示都会被同时用作查询（Query）、键（Key）和值（Value）。

具体来说，对于每个词汇的向量表示x，我们会得到三个向量：q=Wx，k=Wx和v=Wv，其中W是可学习的权重矩阵。

> 疑问：如何学习的 W 权重矩阵？对于所有输入，都是相同的 W 吗？
> 
> 常用 Xavier/Glorot 初始化 W，确保前向传播信号稳定：
> 
> W∼Uniform(−1/根号d_model,1/根号d_model)

然后，我们会计算每个词汇与其他所有词汇之间的注意力分数（Attention Score），该分数是通过将查询向量与键向量的点积得到的。最后，我们会使用这些注意力分数来加权值向量，得到每个词汇的自注意力表示。

> 向量的点积，得到数值，本质是「a向量」在「b向量」上的投影，数值越大，注意力越高（两个向量的关系约密切）。 更多细节，参考[向量点乘与叉乘的概念及几何意义](https://zhuanlan.zhihu.com/p/359975221)



## 3. Q/K/V注意力（Query/Key/Value Attention）

Q/K/V注意力是自注意力机制的核心组成部分。在自注意力机制中，每个词汇的向量表示都会被转换为三个向量：查询向量（Query Vector）、键向量（Key Vector）和值向量（Value Vector）。

这三个向量的作用分别如下：

1. 查询向量（Query Vector）：用于与其他词汇的键向量进行匹配，以计算注意力分数。
2. 键向量（Key Vector）：用于与查询向量进行匹配，以计算注意力分数。
3. 值向量（Value Vector）：用于根据注意力分数生成每个词汇的自注意力表示。

通过Q/K/V注意力的方式，Transformer能够在处理每个词汇时，都能够关注到整个输入序列的信息。这使得模型能够更好地理解和处理文本数据。

































[NingG]:    http://ningg.github.io  "NingG"










