---
layout: post
title: AI ç³»åˆ—ï¼šVector Databases
description: å‘é‡æ•°æ®åº“
published: true
category: AI
---


åŸæ–‡ï¼š[Vector Databases](https://book.premai.io/state-of-open-source-ai/vector-db/)


Work in Progress

This chapter is still being written & reviewed. Please do post links & discussion in the [comments](#vector-db-comments) below, or [open a pull request](https://github.com/premAI-io/state-of-open-source-ai/edit/main/vector-db.md)!

Some ideas:

+   PGVector
    
+   short sections for each of the rows from [the table below](#vector-db-table)
    



**çŸ¢é‡æ•°æ®åº“** `Vector databases` åœ¨è¿‡å»ä¸€å¹´å› ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å…´èµ·è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†`çŸ¢é‡åµŒå…¥`ï¼ˆ**vector embedding**ï¼‰çš„æ¦‚å¿µå·²å­˜åœ¨å¤šå¹´ã€‚

1. åœ¨è¿›è¡Œå›¾åƒåˆ†ç±»æ—¶ï¼Œç¥ç»ç½‘ç»œæå–çš„â€œç‰¹å¾â€å°±æ˜¯â€œçŸ¢é‡åµŒå…¥â€ã€‚
2. è¿™äº›çŸ¢é‡åµŒå…¥ï¼ŒåŒ…å«äº†æœ‰å…³å›¾åƒçš„ç²¾ç‚¼ï¼ˆâ€œå‹ç¼©â€ï¼‰ä¿¡æ¯ã€‚
3. å¯¹äºåŸºäºæ–‡æœ¬çš„æ¨¡å‹ï¼ŒçŸ¢é‡åµŒå…¥æ•è·äº†`å•è¯ä¹‹é—´çš„å…³ç³»`ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£è¯­è¨€ã€‚
4. åµŒå…¥ï¼ˆ`Embeddings`ï¼‰å¯ä»¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ä»¥ä¾›æ—¥åæŸ¥æ‰¾å’Œæ£€ç´¢ä½¿ç”¨ã€‚


Table 11 Comparison of Vector Databases[#](#vector-db-table "Permalink to this table")

| Vector Database | Open Source | Sharding | Supported Distance Metrics | Supported Indices | 
| --- | --- | --- | --- | --- | 
| [weaviate/weaviate](https://github.com/weaviate/weaviate) | ğŸŸ¢ Yes | ğŸŸ¢ Yes | cosine, dot, L2 squared, hamming, manhattan | HNSW, HNSW-PQ | 
| [qdrant/qdrant](https://github.com/qdrant/qdrant) | ğŸŸ¢ Yes | ğŸŸ¢ Yes | cosine, dot, euclidean | HNSW | 
| [milvus-io/milvus](https://github.com/milvus-io/milvus) | ğŸŸ¢ Yes | ğŸŸ¢ Yes | cosine, dot, euclidean, jaccard, hamming | HNSW, FLAT, IVF-FLAT, IVF-PQ | 
| [RedisVentures/redisvl](https://github.com/RedisVentures/redisvl) | ğŸŸ¢ Yes | ğŸŸ¢ Yes | cosine, inner product, L2 | HNSW, FLAT | 
| [chroma-core/chroma](https://github.com/chroma-core/chroma) | ğŸŸ¢ Yes | ğŸ”´ No | cosine, inner product, L2 | HNSW | 
| [Pinecone](https://www.pinecone.io) | ğŸ”´ No | ğŸŸ¢ Yes | cosine, dot, euclidean | HNSW, FLAT, LSH, PQ |


## LLM Embeddings[#](#llm-embeddings "Permalink to this heading")


å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ï¼ˆä¾‹å¦‚ç»´åŸºç™¾ç§‘ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚å½“æ¨¡å‹å¤„ç†è¿™äº›æ–‡æœ¬æ—¶ï¼Œå®ƒæ ¹æ®`å•è¯`åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æ³•ï¼Œå¯¹å•è¯åšäº†æ ‡è¯†ã€‚

1. é«˜ç»´å‘é‡ï¼šéšç€æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå®ƒå°†æ¯ä¸ª`å•è¯`è¡¨ç¤ºä¸ºä¸€ä¸ª`é«˜ç»´å‘é‡`ï¼Œé€šå¸¸å…·æœ‰æ•°ç™¾æˆ–æ•°åƒä¸ªç»´åº¦ã€‚å‘é‡ä¸­çš„å€¼ç¼–ç ï¼Œä»£è¡¨äº†å•è¯çš„å«ä¹‰ã€‚
2. è¿‘ä¼¼è¯­ä¹‰ï¼šåœ¨å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œè®­ç»ƒåï¼Œå…·æœ‰`ç›¸ä¼¼å«ä¹‰`çš„å•è¯åœ¨`å‘é‡ç©ºé—´`ä¸­æ›´åŠ `æ¥è¿‘`ã€‚
3. æ³›åŒ–èƒ½åŠ›ï¼šäº§ç”Ÿçš„å•è¯å‘é‡ï¼Œä½“ç°å‡ºäº†å•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸­å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›é¢„è®­ç»ƒçš„åµŒå…¥ï¼Œè¢«ç”¨åœ¨åˆå§‹åŒ–åƒBERTè¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€å±‚ã€‚

æ€»ä¹‹ï¼Œé€šè¿‡åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œæœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªä¸“é—¨è®¾è®¡æ¥`æ•æ‰å•è¯ä¹‹é—´å…³ç³»`çš„æ¨¡å‹ï¼Œå³**å‘é‡åµŒå…¥**ï¼ˆvector embeddingsï¼‰ã€‚



> å¤§å‹è¯­è¨€æ¨¡å‹å°±åƒä¸€ä½è¯­è¨€ä¸“å®¶ï¼Œé€šè¿‡é˜…è¯»å¤§é‡æ–‡ç« ï¼Œå­¦ä¼šäº†å¦‚ä½•ç†è§£å•è¯ã€‚
> 
> å½“æ¨¡å‹è¯»æ–‡ç« æ—¶ï¼Œå®ƒä¼šè§‚å¯Ÿå•è¯åœ¨æ–‡ç« ä¸­çš„ç”¨æ³•ï¼Œå¹¶æ ¹æ®å®ƒä»¬åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å…³ç³»ç»™æ¯ä¸ªå•è¯æ‰“ä¸Šæ ‡ç­¾ã€‚
> 
> è¿™å°±å¥½åƒå°†æ¯ä¸ªå•è¯éƒ½æ”¾åœ¨ä¸€ä¸ªç‰¹æ®Šçš„ç›’å­é‡Œï¼Œè¿™ä¸ªç›’å­é‡Œæœ‰å¾ˆå¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬è¿™ä¸ªå•è¯çš„æ„æ€ä»¥åŠå®ƒå’Œå…¶ä»–å•è¯ä¹‹é—´çš„è”ç³»ã€‚
> 
> è¿™æ ·ï¼Œæ¨¡å‹å­¦åˆ°çš„è¿™äº›æ ‡ç­¾ï¼ˆæˆ–ç›’å­ï¼‰å¯ä»¥å¸®åŠ©å®ƒæ›´å¥½åœ°ç†è§£è¯­è¨€ï¼Œå¹¶åœ¨éœ€è¦æ—¶åšå‡ºæ›´æ™ºèƒ½çš„å›ç­”ã€‚






## Turning text into embeddings[#](#turning-text-into-embeddings "Permalink to this heading")

![](/images/ai-series/premAI/vector-databases-embedding.jpeg)

Fig. 62 Vector Embeddings[#](#vector-database-embeddings "Permalink to this image")

Letâ€™s take the sentence from the image above as an example: â€œ*I want to adopt a puppy*â€

1.  Each word in the sentence is mapped to its corresponding vector representation using the pre-trained word embeddings. For example, the word â€œadoptâ€ may map to a 300-dimensional vector, â€œpuppyâ€ to another 300-dim vector, and so on.
    
2.  The sequence of word vectors is then passed through the neural network architecture of the language model.
    
3.  As the word vectors pass through the model, they interact with each other and get transformed by mathematical functions. This allows the model to interpret the meaning of the full sequence.
    
4.  The output of the model is a new vector that represents the embedding for the full input sentence. This sentence embedding encodes the semantic meaning of the entire sequence of words.
    

Many closed-source models like [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) from OpenAI and the [embeddings model](https://docs.cohere.com/docs/embeddings) from Cohere allow developers to convert raw text into vector embeddings. Itâ€™s important to note that the models used to generate vector embeddings are NOT the same models used for text generation.

> ç”Ÿæˆ`è¯å‘é‡`çš„æ¨¡å‹ä¸ç”¨äº`æ–‡æœ¬ç”Ÿæˆ`çš„æ¨¡å‹å¹¶ä¸ç›¸åŒã€‚
> 
> 1. è¯å‘é‡æ¨¡å‹ï¼šæ—¨åœ¨å­¦ä¹ è¯è¯­çš„è¯­ä¹‰å…³ç³»å’Œä¸Šä¸‹æ–‡è¡¨ç¤º
> 2. æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼šæ˜¯ç”¨æ¥æ ¹æ®å­¦ä¹ åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆæ–‡æœ¬ã€‚
> 
> è¿™ä¸¤ç§æ¨¡å‹çš„ç›®çš„å’ŒåŠŸèƒ½æ˜¯ä¸åŒçš„ã€‚

Embeddings vs Text Generation

+   For NLP, embeddings are trained on a language modeling objective. This means they are trained to predict surrounding words/context, not to generate text.
    
+   Embedding models are encoder-only models without decoders. They output an embedding, not generated text.
    
+   Generation models like GPT-2/3 have a decoder component trained explicitly for text generation.
    

## Vector Databases[#](#id1 "Permalink to this heading")

çŸ¢é‡æ•°æ®åº“`Vector databases`ï¼Œå¯ä»¥é«˜æ•ˆå­˜å‚¨å’Œæœç´¢ **çŸ¢é‡åµŒå…¥**ï¼ˆ`vector embeddings`ï¼‰ã€‚


### Calculating distance between vectors[#](#calculating-distance-between-vectors "Permalink to this heading")

å¤§å¤šæ•°çŸ¢é‡æ•°æ®åº“ï¼Œæ”¯æŒä¸‰ç§ä¸»è¦çš„`è·ç¦»åº¦é‡`ï¼š

1. æ¬§æ°è·ç¦» [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)ï¼šå‘é‡ç©ºé—´ä¸­ï¼Œä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿è·ç¦»
1. ä½™å¼¦ç›¸ä¼¼åº¦ [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)ï¼šä¸¤ä¸ªå‘é‡ä¹‹é—´å¤¹è§’çš„`ä½™å¼¦å€¼` - ä½™å¼¦å€¼è¶Šå¤§ï¼Œå‘é‡è¶Šæ¥è¿‘
1. ç‚¹ç§¯ [Dot product](https://en.wikipedia.org/wiki/Dot_product)ï¼š`ä½™å¼¦ç›¸ä¼¼åº¦`å’Œå‘é‡é•¿åº¦ï¼ˆå¤§å°ï¼‰çš„ä¹˜ç§¯ - ç‚¹ç§¯è¶Šå¤§ï¼Œå‘é‡è¶Šæ¥è¿‘

    

![](/images/ai-series/premAI/vector-databases-vector-distances.jpeg)

Fig. 63 [Vector Distance Metrics](https://weaviate.io/blog/what-is-a-vector-database)[#](#vector-database-vector-distances "Permalink to this image")

## Vector Indexing[#](#vector-indexing "Permalink to this heading")

Even though vector databases can contain metadata in the form of JSON objects, the primary type of data is `vectors`. Unlike relational databases or NoSQL databases, vector databases optimise operations to make reading and writing vectors as fast as possible.

With vector databases, there are two different concepts of `indexing` and `search algorithms`, both of which contribute to the overall performance. In many situations, choosing a vector index involves a trade-off between accuracy (precision/recall) and speed/throughput \[[148](../references/#id65 "Prashanth Rao. Vector databases: not all indexes are created equal. 2023. URL: https://thedataquarry.com/posts/vector-db-3.")\]. There are two primary factors that help organise an index:

1.  The underlying data structureï¼Œåº•å±‚çš„æ•°æ®ç»“æ„
    
2.  Level of compressionï¼Œå‹ç¼©ç‡
    

![](/images/ai-series/premAI/vector-databases-indexing-diagram.png)

Fig. 64 [Vector Indexing](https://thedataquarry.com/posts/vector-db-3)[#](#vector-database-indexing-diagram "Permalink to this image")

### Hash-based Indexing[#](#hash-based-indexing "Permalink to this heading")

**å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ**ï¼ˆ[Locality-Sensitive Hashing (LSH)](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing)ï¼‰ä½¿ç”¨å“ˆå¸Œå‡½æ•°ï¼Œå°†`ç›¸ä¼¼çš„å‘é‡`åˆ†æ¡¶åˆ°`å“ˆå¸Œè¡¨`ä¸­ã€‚æŸ¥è¯¢å‘é‡ä¹Ÿä½¿ç”¨`ç›¸åŒçš„å“ˆå¸Œå‡½æ•°`è¿›è¡Œå“ˆå¸Œå¤„ç†ï¼Œå¹¶ä¸å·²ç»å­˜åœ¨äºè¡¨ä¸­çš„å…¶ä»–å‘é‡è¿›è¡Œæ¯”è¾ƒã€‚

è¿™ç§æ–¹æ³•æ¯”åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè¯¦å°½æœç´¢è¦å¿«å¾—å¤šï¼Œå› ä¸ºæ¯ä¸ªå“ˆå¸Œè¡¨ä¸­çš„å‘é‡æ¯”æ•´ä¸ªå‘é‡ç©ºé—´ä¸­çš„å‘é‡è¦å°‘ã€‚è™½ç„¶è¿™ç§æŠ€æœ¯`éå¸¸å¿«é€Ÿ`ï¼Œä½†ç¼ºç‚¹æ˜¯å®ƒçš„`å‡†ç¡®æ€§ä¸å¤Ÿ`ã€‚`LSH`æ˜¯ä¸€ç§`è¿‘ä¼¼æ–¹æ³•`ï¼Œå› æ­¤ï¼Œæ›´å¥½çš„å“ˆå¸Œå‡½æ•°ä¼šäº§ç”Ÿæ›´å¥½çš„`è¿‘ä¼¼ç»“æœ`ï¼Œä½†ç»“æœ`ä¸ä¼šæ˜¯ç¡®åˆ‡çš„ç­”æ¡ˆ`ã€‚


### Tree-based Indexing[#](#tree-based-indexing "Permalink to this heading")

åŸºäºæ ‘çš„ç´¢å¼•ï¼Œä½¿ç”¨è¯¸å¦‚äºŒå‰æ ‘ä¹‹ç±»çš„æ•°æ®ç»“æ„ï¼Œè¿›è¡Œå¿«é€Ÿæœç´¢ã€‚æ ‘ä»¥ä¸€ç§æ–¹å¼åˆ›å»ºï¼Œä½¿ç›¸ä¼¼çš„å‘é‡åˆ†ç»„åœ¨åŒä¸€å­æ ‘ä¸­ã€‚[spotify/annoy](https://github.com/spotify/annoy)ï¼ˆApproximate Nearest Neighbour Oh Yeahï¼‰ä½¿ç”¨`äºŒå‰æ ‘``æ£®æ—`æ¥æ‰§è¡Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€‚

1. Annoy åœ¨é«˜ç»´æ•°æ®ä¸­è¡¨ç°è‰¯å¥½ï¼Œå…¶ä¸­è¿›è¡Œç²¾ç¡®çš„æœ€è¿‘é‚»æœç´¢å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚
2. ä½¿ç”¨è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯`å»ºç«‹ç´¢å¼•`å¯èƒ½éœ€è¦`å¤§é‡æ—¶é—´`ã€‚
3. æ¯å½“æ”¶åˆ°æ–°çš„æ•°æ®ç‚¹æ—¶ï¼Œç´¢å¼•æ— æ³•å³æ—¶é‡ç»„ï¼Œå¿…é¡»ä»å¤´å¼€å§‹é‡å»ºæ•´ä¸ªç´¢å¼•ã€‚

### Graph-based Indexing[#](#graph-based-indexing "Permalink to this heading")

ä¸åŸºäºæ ‘çš„ç´¢å¼•ç±»ä¼¼ï¼ŒåŸºäºå›¾çš„ç´¢å¼•é€šè¿‡è¿æ¥ç›¸ä¼¼çš„æ•°æ®ç‚¹æ¥è¿›è¡Œåˆ†ç»„ã€‚å½“å°è¯•åœ¨é«˜ç»´ç©ºé—´ä¸­æœç´¢å‘é‡æ—¶ï¼ŒåŸºäºå›¾çš„ç´¢å¼•éå¸¸æœ‰ç”¨ã€‚[HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw)ï¼ˆHierarchical Navigable Small Worldï¼‰æ˜¯ä¸€ç§æµè¡Œçš„åŸºäºå›¾çš„ç´¢å¼•ï¼Œæ—¨åœ¨åœ¨æœç´¢é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›å¹³è¡¡ã€‚

![](/images/ai-series/premAI/vector-databases-hnsw-diagram.png)

Fig. 65 [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw)[#](#vector-databases-hnsw-diagram "Permalink to this image")


HNSWåˆ›å»ºäº†ä¸€ä¸ªåˆ†å±‚å›¾ï¼Œé¡¶å±‚åŒ…å«æœ€å°‘çš„ç‚¹ï¼Œåº•å±‚åŒ…å«æœ€å¤šçš„ç‚¹ \[[149](../references/#id68 "David Gutsch. Vector databases: understanding the algorithm (part 3). 2023. URL: https://medium.com/@david.gutsch0/vector-databases-understanding-the-algorithm-part-3-bc7a8926f27c.")\]ã€‚

1. å½“è¾“å…¥æŸ¥è¯¢è¿›å…¥æ—¶ï¼Œé€šè¿‡æœ€è¿‘é‚»ç®—æ³•æœç´¢é¡¶å±‚ã€‚
2. å›¾ä¼šé€å±‚å‘ä¸‹éå†ã€‚
3. åœ¨æ¯ä¸€å±‚ï¼Œæœ€è¿‘é‚»ç®—æ³•è¢«è¿è¡Œä»¥æ‰¾åˆ°ä¸è¾“å…¥æŸ¥è¯¢æœ€æ¥è¿‘çš„ç‚¹ã€‚
4. ä¸€æ—¦åˆ°è¾¾åº•å±‚ï¼Œå°†è¿”å›ä¸è¾“å…¥æŸ¥è¯¢æœ€è¿‘çš„ç‚¹ã€‚

åŸºäºå›¾çš„ç´¢å¼•éå¸¸é«˜æ•ˆï¼Œå› ä¸ºå®ƒå…è®¸åœ¨æ¯ä¸€å±‚ç¼©å°æœç´¢èŒƒå›´ä»è€Œåœ¨é«˜ç»´ç©ºé—´ä¸­æœç´¢ã€‚ç„¶è€Œï¼Œé‡æ–°ç´¢å¼•å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¯èƒ½éœ€è¦é‡æ–°åˆ›å»ºæ•´ä¸ªå›¾\[[149](../references/#id68 "David Gutsch. Vector databases: understanding the algorithm (part 3). 2023. URL: https://medium.com/@david.gutsch0/vector-databases-understanding-the-algorithm-part-3-bc7a8926f27c.")\]ã€‚

### Inverted File Index[#](#inverted-file-index "Permalink to this heading")

IVF(Inverted File Index)é€šè¿‡å°†`æ•°æ®é›†`è¿›è¡Œ`åˆ†åŒº`å¹¶ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºä¸€ä¸ª`ä¸­å¿ƒç‚¹`ï¼ˆéšæœºç‚¹ï¼‰æ¥ç¼©å°æœç´¢ç©ºé—´ã€‚

1. è¿™äº›`ä¸­å¿ƒç‚¹`ï¼Œé€šè¿‡`K-Means`ç®—æ³•è¿›è¡Œæ›´æ–°ã€‚
2. ä¸€æ—¦ç´¢å¼•è¢«å¡«å……ï¼Œ`æœ€è¿‘é‚»ç®—æ³•`ä¼šæ‰¾åˆ°ç¦»`è¾“å…¥æŸ¥è¯¢`æœ€è¿‘çš„`ä¸­å¿ƒç‚¹`ï¼Œç„¶ååªåœ¨è¯¥åˆ†åŒºä¸­è¿›è¡Œæœç´¢ã€‚

è™½ç„¶IVFåœ¨åˆ›å»ºç´¢å¼•åï¼Œæœç´¢ç›¸ä¼¼ç‚¹æ—¶éå¸¸é«˜æ•ˆï¼Œä½†åˆ›å»ºåˆ†åŒºå’Œä¸­å¿ƒç‚¹çš„è¿‡ç¨‹å¯èƒ½ä¼šç›¸å½“æ…¢ã€‚

### Vector Compression[#](#vector-compression "Permalink to this heading")

Vectors can take up a lot of memory in terms of storage. High dimensional data adds to this problem which can end up making vector search slow and difficult to manage. To tackle this issue, compression is used to reduce the overall footprint of the vector while still retaining the core structure of the data.

> å‘é‡å‹ç¼©ï¼Œä½†ä¸æŸå¤±ä¿¡æ¯é‡.

There are two kinds of compression techniques:

+   **Flat**
    
+   **Product Quantisation (PQ)**
    

Flat compression does not modify the vectors and keeps the original structure. When an input query comes in a kNN search is done to find the exact match between the input vector and the vectors present in the vector database. This leads to a high level of accuracy, but it comes at the cost of speed. The search time increases linearly as the size of the dataset grows. When dealing with larger datasets, flat will likely yield poor results in terms of latency.

On the other hand, product quantisation reduces the memory footprint of the original vectors by decreasing the number of dimensions. It splits the original vector into chunks and gives each chunk an id. These chunks are created in a way that the distance between them can be calculated efficiently.

Product Quantisation works well for large datasets and high-dimension spaces. It can greatly speed up the nearest neighbour search and reduce the overall memory footprint by ~97%. The downside of using this compression technique is that it can lead to lower accuracy and recall \[[150](../references/#id66 "Inc Pinecone Systems. Product quantisation: compressing high-dimensional vectors by 97%. 2023. URL: https://www.pinecone.io/learn/series/faiss/product-quantization.")\].


æœ‰ä¸¤ç§å‹ç¼©æŠ€æœ¯ï¼š

1.**å¹³å¦å‹ç¼©ï¼ˆFlatï¼‰**

**å¹³å¦å‹ç¼©**`ä¸ä¿®æ”¹å‘é‡`å¹¶ä¿æŒåŸå§‹ç»“æ„ã€‚

* å½“è¾“å…¥æŸ¥è¯¢æ—¶ï¼Œè¿›è¡Œ`Kè¿‘é‚»æœç´¢`ï¼ˆKNNï¼‰ä»¥æ‰¾åˆ°è¾“å…¥å‘é‡ä¸å‘é‡æ•°æ®åº“ä¸­å‘é‡çš„ç²¾ç¡®åŒ¹é…ä½ç½®ã€‚
* è¿™æä¾›äº†é«˜æ°´å¹³çš„å‡†ç¡®æ€§ï¼Œä½†ä»¥é€Ÿåº¦ä¸ºä»£ä»·ã€‚
* éšç€æ•°æ®é›†å¤§å°çš„å¢é•¿ï¼Œæœç´¢æ—¶é—´ä¼šçº¿æ€§å¢åŠ ã€‚å½“å¤„ç†è¾ƒå¤§æ•°æ®é›†æ—¶ï¼Œå¹³å¦å‹ç¼©å¯èƒ½å¯¼è‡´å»¶è¿Ÿæ€§èƒ½è¾ƒå·®ã€‚

2.**ä¹˜ç§¯é‡åŒ–ï¼ˆProduct Quantisationï¼ŒPQï¼‰**

* **ä¹˜ç§¯é‡åŒ–**é€šè¿‡`å‡å°‘ç»´åº¦`æ¥å‡å°åŸå§‹å‘é‡çš„å†…å­˜å ç”¨ã€‚
* å®ƒå°†åŸå§‹å‘é‡`åˆ†å‰²`æˆå—ï¼Œå¹¶ä¸ºæ¯ä¸ªå—åˆ†é…ä¸€ä¸ª`ID`ã€‚
* åˆ›å»ºå—çš„æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆè®¡ç®—`å®ƒä»¬ä¹‹é—´çš„è·ç¦»`ã€‚
* ä¹˜ç§¯é‡åŒ–ï¼Œé€‚ç”¨äº`å¤§å‹æ•°æ®é›†`å’Œé«˜ç»´ç©ºé—´ã€‚
* å®ƒå¯ä»¥æå¤§åœ°åŠ é€Ÿæœ€è¿‘é‚»æœç´¢ï¼Œå¹¶å°†æ•´ä½“å†…å­˜å ç”¨é™ä½çº¦97%ã€‚
* ä½¿ç”¨è¿™ç§å‹ç¼©æŠ€æœ¯ï¼Œç¼ºç‚¹æ˜¯å¯èƒ½ä¼šå¯¼è‡´è¾ƒ`ä½çš„å‡†ç¡®æ€§`å’Œå¬å›ç‡ã€‚

## Searching Algorithms[#](#searching-algorithms "Permalink to this heading")

Vector indexing is more about selecting the underlying data structure to store the vectors. Vector searching is about picking the algorithm used to search on that data structure.

A basic algorithm used for vector search is kNN (K-Nearest Neighbors). kNN works by calculating the distance between the input vector and all of the other vectors inside the vector database. This algorithm does not scale well as the number of vectors increases, because as the number of vectors increases so does the search time.

There is a more efficient search algorithm commonly used by vector databases called ANN(Approximate Nearest Neighbors). ANN works by pre-computing the distance between the vectors and storing them in a way so that similar vectors are placed closer to each other.

By grouping or clustering similar vectors, the algorithm can quickly narrow down the search space without wandering further away from the input query.

## Popular Use-Cases[#](#popular-use-cases "Permalink to this heading")

A common use case for vector databases is search. Whether itâ€™s searching for similar text or images, this tool can efficiently find the data you are looking for.

![](https://static.premai.io/book/vector-databases-llm-prompting.png)

Fig. 66 [LLM prompt injection with vector databases](https://weaviate.io/blog/private-llm)[#](#vector-databases-llm-prompting "Permalink to this image")


åœ¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œå‘é‡æ•°æ®åº“ç»å¸¸ç”¨äºä»ç”¨æˆ·çš„æŸ¥è¯¢ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œä»¥åœ¨LLMçš„æç¤ºä¸­ä½¿ç”¨ã€‚`å‘é‡æ•°æ®åº“`å¯ä»¥ä½œä¸ºLLMçš„é•¿æœŸè®°å¿†ï¼Œå› æ­¤åªæœ‰ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„éƒ¨åˆ†ï¼Œè¢«æ³¨å…¥åˆ°æç¤ºä¸­ã€‚

å¦ä¸€ä¸ªç”¨ä¾‹æ˜¯æ¨èå¼•æ“ã€‚æ¨èçš„æœ¬è´¨æ˜¯å¯»æ‰¾ç›¸ä¼¼çš„äº§å“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…³ç³»å‹æˆ–NoSQLæ•°æ®åº“æ•ˆæœä¸ä½³ï¼Œå› ä¸ºä¸éœ€è¦ç²¾ç¡®åŒ¹é…ã€‚å‘é‡æ•°æ®åº“å·²è¢«ç”¨äºå„ç§æ¨èï¼Œä»ç”µå½±åˆ°ç”µå­å•†åŠ¡äº§å“ã€‚

## Limitations[#](#limitations "Permalink to this heading")

While there are many advantages to using vector databases in certain applications, there are also a few issues to be aware of:

+   Data structure
    
    +   Vector databases are optimised to work with only vector data. The underlying data structures may not be suitable for working with tabular or JSON data.
        
    +   For this reason, vector databases should not be used as a replacement for other types of databases as they lack many of the features such as being [ACID-compliant](https://www.mongodb.com/databases/acid-compliance).
        
+   Debugging difficulty
    
    +   To humans a vector looks like a random list of numbers. These numbers donâ€™t make any sense to us, so it becomes difficult to interpret what this vector represents.
        
    +   Unlike a relational database where we can read the data in each column, we cannot simply read the vector. This makes vector data difficult to debug, as we have to rely on algorithms and metrics to make sense of the data.
        
+   Indexing issues
    
    +   The way a vector database is indexed is crucial to its search performance.
        
    +   However, due to the way some indices are designed it can be quite challenging to modify or delete data. For some indices, the entire underlying data structure needs to be re-formatted when data changes are made.
        

## Future[#](#future "Permalink to this heading")

+   Vector databases provide a unique solution to problems that are not sufficiently addressed by relational or NoSQL databases
    
+   Instead of competing directly against prior databases, it has carved out its own category in the tech stack
    
+   Advancements in indexing and searching algorithms will make vector databases faster and cheaper
    
+   80â€“90% of the data daily generated on the internet is unstructured \[[151](../references/#id67 "Marcel Deer. How much data in the world is unstructured? 2023. URL: https://www.unleash.so/a/answers/database-management/how-much-data-in-the-world-is-unstructured.")\]. Most of it is in the form of text, images, and video. Vector databases can help extract value from unstructured data, whether is improving LLM accuracy, image similarity, or product recommendations.
    

åœ¨å¯é¢„è§çš„æœªæ¥ï¼Œå‘é‡æ•°æ®åº“å°†ä¼šæŒç»­å­˜åœ¨ã€‚å®ƒä»¬ä¼¼ä¹ä¸å¤ªå¯èƒ½å–ä»£ä¼ ç»Ÿæ•°æ®åº“æˆ–è¢«å…¶æ‰€å–ä»£ï¼Œå› ä¸ºå®ƒä»¬å„è‡ªå…·æœ‰ä¸åŒçš„ç”¨é€”ã€‚è¿™é¡¹æŠ€æœ¯æœ€ç»ˆå°†æˆä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯æ ˆä¸­çš„ä¸»æµç»„æˆéƒ¨åˆ†ã€‚






























[NingG]:    http://ningg.github.io  "NingG"
[premAI]:		https://book.premai.io/state-of-open-source-ai/








