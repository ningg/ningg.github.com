---
layout: post
title: AI ç³»åˆ—ï¼šDesktop Apps
description: æ¡Œé¢åº”ç”¨
published: true
category: AI
---


åŸæ–‡ï¼š[Desktop Apps](https://book.premai.io/state-of-open-source-ai/desktop-apps/)



ChatGPT å’Œ GPT-4 åœ¨è¿‡å»åŠå¹´å†…å¸­å·äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä½†å¼€æºæ¨¡å‹ä¹Ÿåœ¨è¿å¤´èµ¶ä¸Šã€‚è¦è¾¾åˆ° OpenAI æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œè¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä¸éƒ¨ç½²åœ¨äº‘æœåŠ¡å™¨ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒChatGPT å’Œ GPT-4 æ˜¾ç„¶æ›´èƒœä¸€ç­¹ï¼Œå› ä¸ºæ¯ä¸ª OpenAI API è¯·æ±‚çš„æˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œè€Œäº‘æœåŠ¡ï¼ˆå¦‚ AWSã€Azure å’Œ Google Cloudï¼‰ä¸Šçš„æ¨¡å‹æ‰˜ç®¡æˆæœ¬è¾ƒé«˜ã€‚ä½†å¯¹äºæŸäº›ä¸šåŠ¡æ¡ˆä¾‹ï¼Œå¼€æºæ¨¡å‹å§‹ç»ˆæ¯”åƒ ChatGPT/GPT-4 è¿™æ ·çš„å°é—­ API æ›´æœ‰ä»·å€¼ã€‚åƒæ³•å¾‹ã€åŒ»ç–—ä¿å¥ã€é‡‘èç­‰è¡Œä¸šçš„äººå¯¹æ•°æ®å’Œå®¢æˆ·éšç§æœ‰æ‰€é¡¾è™‘ã€‚

ä¸€ä¸ªæ–°è€Œä»¤äººå…´å¥‹çš„é¢†åŸŸæ˜¯æ”¯æŒåœ¨`æœ¬åœ°è¿è¡Œ`å¼ºå¤§è¯­è¨€æ¨¡å‹çš„`æ¡Œé¢åº”ç”¨ç¨‹åº`(desktop apps)ã€‚å¯ä»¥è¯´ï¼ŒæˆåŠŸçš„æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œåœ¨æŸäº›æ•æ„Ÿæƒ…å†µä¸‹æ¯”åŸºäºäº‘çš„æœåŠ¡æ›´æœ‰ç”¨ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®ã€æ¨¡å‹å’Œåº”ç”¨ç¨‹åºéƒ½å¯ä»¥åœ¨é€šå¸¸å¯ç”¨çš„ç¡¬ä»¶ä¸Šæœ¬åœ°è¿è¡Œã€‚æœ¬æ–‡ï¼Œæˆ‘ä¼šä»‹ç»ä¸€äº›æ–°å…´çš„ `LLM æ¡Œé¢åº”ç”¨`è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸åŒåº”ç”¨çš„ä¼˜åŠ¿ã€å±€é™æ€§ä»¥åŠå·®å¼‚ç‚¹çš„æ¯”è¾ƒã€‚


Table 13 Comparison of Desktop Apps[#](#id9 "Permalink to this table")


| Desktop App | Supported Models | GPU support | Layout | Configuration | Extra Features | OS | Future Roadmap |
 | --- | --- | --- | --- | --- | --- | --- | --- |
 | [LM Studio](#lm-studio) | ğŸŸ¡ [GGML](../model-formats/#id3) | ğŸŸ¢ Yes | Clean, clear tabs. | Hardware config choices (GPU, RAM, etc.). Can choose multiple inference params (temperature, repeat penalty, etc.). | Local server deployments | Windows, Linux, MacOS | Not mentioned |
 | [GPT4All](#gpt4all) | ğŸŸ¡ [GGML](../model-formats/#id3) | ğŸ”´ No | Unclear tabs. | Minimal hardware config options. Can choose inference params. | Contribute & use training data from the GPT4All datalake | Windows, Linux, MacOS | [Building open-source datalake for future model training](https://gpt4all.io) |
 | [koboldcpp](#koboldcpp) | ğŸŸ¡ [GGML](../model-formats/#id3) | ğŸ”´ No | Cluttered UI. | Some hardware config options. Unique inference/app params e.g. [scenarios.](https://github.com/LostRuins/koboldcpp) | Cool story, character, and adventure modes | Windows, Linux, MacOS | Not mentioned |
 | [local.ai](#local-ai) | ğŸŸ¡ [GGML](../model-formats/#id3) | ğŸ”´ No | Clear tabs. | Minimal hardware config options. Can choose inference params. | Light/dark modes | Windows, Linux, MacOS | [Text-to-audio, OpenAI functions](https://github.com/louisgv/local.ai) |
 | [Ollama](#ollama) | ğŸ”´ few [GGML](../model-formats/#id3) models | ğŸŸ¡ Yes (metal) | Basic, terminal-based UI. | Multiple hardware configurations, need to save as a file prior to running. Multiple inference params, need to save as a file. | Run from terminal | MacOS | [Windows, Linux support](https://ollama.ai) |


## LM Studio[#](#lm-studio "Permalink to this heading")

LM Studio is an app to run LLMs locally.

### UI and Chat[#](#ui-and-chat "Permalink to this heading")

[LM Studio](https://lmstudio.ai) is a desktop application supported for Windows and Mac OS that gives us the flexibility to run LLMs on our PC. You can download any `ggml` model from the [HuggingFace models hub](https://huggingface.co/models) and run the model on the prompts given by the user.

The UI is pretty neat and well contained:

![https://static.premai.io/book/lm-studio1.png](https://static.premai.io/book/lm-studio1.png)

Fig. 67 LM Studio UI[#](#id10 "Permalink to this image")

Thereâ€™s a search bar that can be used to search for models from the HuggingFace models to power the chat.

![https://static.premai.io/book/lmstudio-search.png](https://static.premai.io/book/lmstudio-search.png)

Fig. 68 LM Studio Model Search[#](#id11 "Permalink to this image")

The Chat UI component is similar to ChatGPT to have conversations between the user and the assistant.

![https://static.premai.io/book/lmstudio-chat-int.png](https://static.premai.io/book/lmstudio-chat-int.png)

Fig. 69 LM Studio Chat Interface[#](#id12 "Permalink to this image")

This is how the `TheBloke/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q5_K_S.bin` responds to a simple conversation starter.

![https://static.premai.io/book/desktopapps-lmstudio-chat.png](https://static.premai.io/book/desktopapps-lmstudio-chat.png)

Fig. 70 LM Studio Chat Example[#](#id13 "Permalink to this image")

### Local Server[#](#local-server "Permalink to this heading")

One useful aspect is the ability to build a Python or Node.js application based on an underlying LLM.

![https://static.premai.io/book/lmstudio-local.png](https://static.premai.io/book/lmstudio-local.png)

Fig. 71 LM Studio Local Server[#](#id14 "Permalink to this image")

This enables the user to build applications that are powered by LLMs and using `ggml` models from the HUggingFace model library (without API key restrictions).

Think of this server like a place where you make API calls to and get the response. The only change is that this is a local server and not a cloud based server. This makes it quite exciting to use the hardware in your system to power the LLM application that you are building.

Letâ€™s spin up the server by hitting the `Start server` buttonğŸ‰. That was a quick one and by default it is served in port `1234` and if you want to make use of some other port then you can edit that left to the `Start server` button that you pressed earlier. There are also few parameters that you can modify to handle the request but for now letâ€™s leave it as default.

Go to any Python editor of your choice and paste the following code by creating a new `.py` file.

```
import openai
\# endpoint:port of your local inference server (in LM Studio)
openai.api\_base\='http://localhost:1234/v1'
openai.api\_key\=''  \# empty
prefix \= "### Instruction:\\n"
suffix \= "\\n\### Response:"

def get\_completion(prompt, model\="local model", temperature\=0.0):
    formatted\_prompt \= f"{prefix}{prompt}{suffix}"
    messages \= \[{"role": "user", "content": formatted\_prompt}\]
    print(f'\\nYour prompt: {prompt}\\n')
    response \= openai.ChatCompletion.create(
        model\=model,
        messages\=messages,
        temperature\=temperature)
    return response.choices\[0\].message\["content"\]

prompt \= "Please give me JS code to fetch data from an API server."
response \= get\_completion(prompt, temperature\=0)
print(f"LLM's response:{response}")

```


This is the code that I ran using the command `python3 <filename>.py` and the results from server logs and terminal produced are shown below:

![https://static.premai.io/book/lmstudio-local-ex.png](https://static.premai.io/book/lmstudio-local-ex.png)

Fig. 72 LM Studio Local Server Example[#](#id15 "Permalink to this image")

### Model Configurations & Tools[#](#model-configurations-tools "Permalink to this heading")

By default we have a few presets already provided by LM studio but we can tweak them and create a preset of our own to be used elsewhere. The parameters that are modifiable are:

+   ğŸ› ï¸ Inference parameters: These gives the flexibility to change the `temperature`, `n_predict`, and `repeat_penalty`
    
+   â†”ï¸ Input prefix and suffix: Text to add right before, and right after every user message
    
+   â‚ Pre-prompt / System prompt: Text to insert at the very beginning of the prompt, before any user messages
    
+   ğŸ“¥ Model initialisation: `m_lock` when turned on will ensure the entire model runs on RAM.
    
+   âš™ï¸ Hardware settings: The `n_threads` parameter is maximum number of CPU threads the model is allowed to consume. If you have a GPU, you can turn on the `n_gpu_layers` parameter. You can set a number between 10-20 depending on the best value, through experimentation.
    

Tools focus on the response and UI of the application. The parameters modifiable are as follows:

+   ğŸ”  `Context overflow policy`: Behaviour of the model for when the generated tokens length exceeds the context window size
    
+   ğŸŒˆ `Chat appearance`: Either plain text (.txt) or markdown (.md)
    
+   ğŸ“ `Conversation notes`: Auto-saved notes for a specific chat conversation
    

### Features[#](#features "Permalink to this heading")

+   ğŸ’ª å……åˆ†åˆ©ç”¨æ‚¨è®¡ç®—æœºçš„æ€§èƒ½æ¥è¿è¡Œæ¨¡å‹ï¼Œå³ï¼Œå¦‚æœæ‚¨çš„è®¡ç®—æœºæ€§èƒ½è¶Šå¼ºå¤§ï¼Œå°±èƒ½å……åˆ†å‘æŒ¥å…¶æ€§èƒ½ã€‚
    
+   ğŸ†• é€šè¿‡ä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥æµ‹è¯•æœ€æ–°çš„æ¨¡å‹ï¼Œæ¯”å¦‚ LLaMa æˆ–å…¶ä»–å…¬å¼€æ‰˜ç®¡åœ¨ HuggingFace ä¸Šçš„æ–°æ¨¡å‹ã€‚æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ MPTã€Starcoderã€Replitã€GPT-Neo-X ç­‰ ggml æ ¼å¼ç±»å‹çš„æ¨¡å‹ã€‚
    
+   ğŸ’» å¯åœ¨ Windows å’Œ Mac å¹³å°ä¸Šä½¿ç”¨ã€‚
    
+   ğŸ”Œ æ¨¡å‹å¯ä»¥å®Œå…¨ç¦»çº¿è¿è¡Œï¼Œå› ä¸ºå®ƒä»¬è¢«ä¸‹è½½å¹¶å­˜å‚¨åœ¨æ‚¨çš„è®¡ç®—æœºæœ¬åœ°ã€‚
    
+   ğŸ’¬ å¯é€šè¿‡èŠå¤©ç•Œé¢æˆ–æœ¬åœ°æœåŠ¡å™¨è®¿é—®åº”ç”¨ã€‚


## GPT4All[#](#gpt4all "Permalink to this heading")

The [GPT4All homepage](https://gpt4all.io) states that

> GPT4All is an ecosystem to train and deploy **powerful** and **customised** large language models that run **locally** on consumer grade CPUs.

### UI and Chat[#](#id1 "Permalink to this heading")

The UI for GPT4All is quite basic as compared to LM Studio â€“ but it works fine.

![https://static.premai.io/book/desktopapps-gpt4all-ui.png](https://static.premai.io/book/desktopapps-gpt4all-ui.png)

Fig. 73 GPT4All UI[#](#id16 "Permalink to this image")

However, it is less friendly and more clunky/ has a beta feel to it. For one, once I downloaded the LLaMA-2 7B model, I wasnâ€™t able to download any new model even after restarting the app.

### Local Server[#](#id2 "Permalink to this heading")

Like LM Studio, there is a support for local server in GPT4All. But it took some time to find that this feature exists and was possible only from the [documentation](https://docs.gpt4all.io). The results seem far better than LM Studio with control over number of tokens and response though it is model dependent. Hereâ€™s the code for the same:

import openai
openai.api\_base \= "http://localhost:4891/v1"
openai.api\_key \= ""
\# Set up the prompt and other parameters for the API request
prompt \= "Who is Michael Jordan?"
model \= "Llama-2-7B Chat"
\# Make the API request
response \= openai.Completion.create(
    model\=model,
    prompt\=prompt,
    max\_tokens\=199,
    temperature\=0.28,
    top\_p\=0.95,
    n\=1,
    echo\=True,
    stream\=False)
\# Print the generated completion
print(response)

Copy to clipboard

The response can be found for the example `prompt`:

![https://static.premai.io/book/gpt4all-ex.png](https://static.premai.io/book/gpt4all-ex.png)

Fig. 74 GPT4All UI Example[#](#id17 "Permalink to this image")

### Model Configurations & Tools[#](#id3 "Permalink to this heading")

As you can see â€“ there is not too much scope for model configuration, and unlike LM Studio â€“ I couldnâ€™t use my GPU here.

![https://static.premai.io/book/desktopapps-gpt4all-modelconfig.png](https://static.premai.io/book/desktopapps-gpt4all-modelconfig.png)

Fig. 75 GPT4All UI Model Configuration[#](#id18 "Permalink to this image")

## koboldcpp[#](#koboldcpp "Permalink to this heading")

[LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) is a fun twist on LLMs â€“ adding game like scenarios and adventures. It supports adding base `ggml` models as the LLM engine, and spinning stories based on user inputs.

### UI and Chat[#](#id4 "Permalink to this heading")

The UI is pretty basic â€“ and you get some surprising answers. Here I ask a simple icebreaker question â€“ and you see that it responds that it is a friendly AI that likes to play games.

![https://static.premai.io/book/desktopapps-koboldcpp-ui.png](https://static.premai.io/book/desktopapps-koboldcpp-ui.png)

Fig. 76 koboldcpp UI[#](#id19 "Permalink to this image")

### Scenarios[#](#scenarios "Permalink to this heading")

You can also enter different sorts of scenarios and modes.

![https://static.premai.io/book/desktopapps-kcpp-scenarios.png](https://static.premai.io/book/desktopapps-kcpp-scenarios.png)

Fig. 77 koboldcpp Scenarios[#](#id20 "Permalink to this image")

Below is the Julius Caesar scenario!

![https://static.premai.io/book/desktopapps-kcpp-jc.png](https://static.premai.io/book/desktopapps-kcpp-jc.png)

Fig. 78 koboldcpp Julius Caesar Chat[#](#id21 "Permalink to this image")

### Model Configuration and Tools[#](#model-configuration-and-tools "Permalink to this heading")

Many of the model configurations are similar to the default that is offered. But there are some interesting twists like story mode, adventure mode, and instruct mode.

![https://static.premai.io/book/desktopapps-kcpp-modes.png](https://static.premai.io/book/desktopapps-kcpp-modes.png)

Fig. 79 koboldcpp Julius Model Configuration[#](#id22 "Permalink to this image")

## [local.ai](https://www.localai.app)[#](#local-ai "Permalink to this heading")

The [local.ai](https://www.localai.app) App from [louisgv/local.ai](https://github.com/louisgv/local.ai) ([not to be confused](https://github.com/louisgv/local.ai/discussions/71) with [LocalAI](../mlops-engines/#localai) from [mudler/LocalAI](https://github.com/mudler/LocalAI)) is a simple application for loading LLMs after you manually download a `ggml` model from online.

### UI and Chat[#](#id5 "Permalink to this heading")

The UI and chat are pretty basic. One bug that I noticed was that it wasnâ€™t possible to load models from the UI â€“ I had to manually download the model and then use the app.

![https://static.premai.io/book/desktopapps-localai-ui.png](https://static.premai.io/book/desktopapps-localai-ui.png)

Fig. 80 [local.ai](https://www.localai.app) UI[#](#id23 "Permalink to this image")

### Model Configuration and Tools[#](#id6 "Permalink to this heading")

Pretty standard prompt related configurations. It appears there is no GPU.

## Ollama[#](#ollama "Permalink to this heading")

[Ollama](https://ollama.ai) is an LLM based conversational chat bot that can be run from a MAC terminal. It is simple to get started. Currently, it is available only for the Mac OS but support for Windows and Linux are coming soon.

### UI and Chat[#](#id7 "Permalink to this heading")

Neat clean and crisp UI, just `>>>` in the terminal and you can paste your prompt. The response time will vary according to the model size but responses are mostly acceptable. I tested the `LLaMA` model which is the most recently supported model and the results were good.

![https://static.premai.io/book/ollama-ex.png](https://static.premai.io/book/ollama-ex.png)

Fig. 81 Ollama Example[#](#id24 "Permalink to this image")

`Note:` It just takes some time initially for the model to download locally, but later whenever you need to access the model there is no lag in accessing the requested model.

### Model Configuration and Tools[#](#id8 "Permalink to this heading")

The list of ~20 models can be accessed [here](https://ollama.ai/library).

They are constantly growing and multiple changes have happened quite recently. It can support models ranging from lite to robust models. It also has special support for specific functionality like performing Mathematical calculations. There is a `WizardMath` model that addresses these use case â€“ read more about this in their official [blog](https://ollama.ai/blog/wizardmath-examples) published by the Ollama team.

### Limitations[#](#limitations "Permalink to this heading")

+   Better response format: There can be a formatted output making use of the terminal features to display the code, text, and images in the latter stage. This will make the output more readable and consistent to the user.
    
+   Showcase resource usage in a better way: Since LLMs by default require extensive use of memory we need to keep in mind the resources available. So while working in a terminal such details will not be explicitly available and can sometimes consume all the memory which can cause the application or the entire system to crash.
    
+   Support for custom models (from local): There is support to load models downloaded from the internet and run them locally by using the command:
    

ollama run "model location in the system"


























[NingG]:    http://ningg.github.io  "NingG"
[premAI]:		https://book.premai.io/state-of-open-source-ai/








