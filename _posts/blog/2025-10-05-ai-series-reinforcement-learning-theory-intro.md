---
layout: post
title: AI 系列：强化学习，理论基础 & 典型算法
description: 场景建模 MDP，价值函数、贝尔曼方程(递归关系)，两类典型算法 Value-based、Policy-based 等.
published: true
categories: AI
---

## 0.概要

现在 RL 是 AI 领域非常热门的方向之一，特别是高级智能的典型训练方法。

RL，全程 Reinforcement Learning，强化学习，是一种通过与环境交互，学习最优策略的机器学习方法。

机器学习方法，可以分为 3 大类：

| 类型        | 学习方式            | 示例        |
| --------- | --------------- | --------- |
| **监督学习**  | 已知输入+正确输出 → `学映射` | 猫狗图片分类    |
| **无监督学习** | 只给输入 → 学结构、`分布`   | 聚类        |
| **强化学习**  | 通过互动和反馈 → 学`最优策略` | 自动驾驶、围棋AI |


非常有必要，集中学习下 RL 的理论基础 & 典型算法。

过去几周，线闲暇时间，集中学习了一遍，点击下面查看细节：

* [RL 强化学习，数学理论、典型算法.](https://github.com/ningg/reinforcement-learning-theory-and-practice) ：这是个 GitHub 项目。

下面简单列些要点。

## 1.定义

> **强化学习（Reinforcement Learning, RL）** 是一种让「智能体（`agent`）」通过「试错」与「环境（`environment`）」互动，从而`学习`「最佳`行为策略`」的机器学习方法。



## 2.场景建模 MDP

我们通常假设环境满足 **Markov Decision Process (MDP)**：

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)
$$

* $$\mathcal{S}$$：状态空间
* $$\mathcal{A}$$：动作空间
* $$P(s'\|s,a)$$：状态转移概率，表示在状态 $$s$$ 下，采取动作 $$a$$ 后，转移到状态 $$s'$$ 的概率。
* $$R(s,a)$$：即时奖励函数
* $$\gamma$$：折扣系数

**马尔可夫性**的含义是：


$$
P(s_{t+1}\|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}\|s_t, a_t)
$$

—— `未来只依赖当前`状态和动作，`不依赖过去`历史。



## 3. RL 核心目标

RL 目标：最大化`长期收益`。


强化学习的核心目标是：

$$
\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big]
$$


解释一下每个部分的意义：

* `argmax`：argument max 求的是`表达式`使达到最大值的`自变量` $$\pi$$，记作 $$\pi^*$$。
* $$\pi$$ ：策略（policy），定义在每个状态 (s) 下，采取动作 (a) 的`概率分布`  $$\pi(a\|s)$$ 。
* $$R_t$$ ：在第 (t) 步获得的`即时奖励`，只是「当前步骤的即时奖励」，并不包含未来的奖励。
* $$\gamma \in (0,1]$$：`折扣因子`（discount factor），控制「未来奖励」的重要程度。
* 目标是让策略 $$\pi$$ 产生的**期望累积奖励**最大化。

> 这就像是一个玩家，不只想每次得分高，而是希望整局游戏的“总得分”最高。

## 4.理论基础：价值函数 & 贝尔曼方程


### 4.1.价值函数：衡量策略优劣的指标

强化学习的核心是学习 `价值函数`（Value Function）。

两个函数：

* V 函数：只看`状态` $$s$$ 的好坏（Value）；
* Q 函数：看`状态 + 动作` $$(s,a)$$ 的好坏（Quality）。


#### 4.1.1.状态价值函数 Value

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0 = s\Big]
$$

→ 表示：在状态 $$s$$ 下，按策略 $$\pi$$ 行动，能期望获得的长期收益。

* $$R_t$$：此刻的`瞬时快乐`；
* $$\sum_{t=0}^\infty \gamma^t R_t$$：一生的`总幸福`，只是未来的快乐会打个折（ $$\gamma < 1$$ ）。


所以，在强化学习里我们通常还会定义一个**回报（Return）**：

$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$

→ 也就是 **从时间 (t) 开始往后看** 的总收益。
而价值函数 $$V^{\pi}(s_t)$$ 的定义，也可写为：

$$
V^{\pi}(s_t) = \mathbb{E}[G_t | s_t]
$$

#### 4.1.2.状态-动作价值函数（Q 函数）

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0=s, a_0=a\Big]
$$

→ 表示：从状态 $$s$$ 出发，先做动作 $$a$$，再按策略 $$\pi$$ 行动的期望收益。 Q 来源于 Quality 质量/价值。

它们之间关系为： `状态价值V`，等于 `状态-动作价值Q` 的`加权平均`。

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \cdot Q^{\pi}(s,a)
$$


### 4.2.贝尔曼方程：递归关系

这是 `RL` 理论的核心方程。

$$
Q^{\pi}(s,a) 
$$
$$
= \mathbb{E}\big[R(s,a) + \gamma V_{s'} \big]
$$
$$
= \mathbb{E}\big[R(s,a) + \gamma \sum_{a'} \pi(a'|s') \cdot Q^{\pi}(s',a') \big]
$$

→ 表示`当前动作`价值等于：

* 当前的`即时奖励`  $$R(s,a)$$ ；
* 加上 `下一状态的价值` 的`折扣期望`， $$\gamma$$ 为折扣系数
* 其中： $$s'$$ 是 $$s$$ 采取动作 $$a$$ 后转移到的新状态， $$a'$$ 是 $$s'$$ 可能采取的动作。

> **Tips**: **动作价值**（`Q 函数`），包含了 **及时奖励** + **下一个状态价值** (`V 函数`)的折扣期望.

而对于`最优策略` $$\pi^*$$，我们得到 **贝尔曼最优方程**：

$$
Q^*(s,a)
$$

$$
=  \mathbb{E}\big[R(s,a) + \gamma \max_{a'} Q^*(s',a') \big]
$$

这就是 `Q-learning` 的理论基础。



## 5.学习方法的两大路线

RL 学习方法的两大路线：


| 类型     | 代表算法       | 思想      | 关键点      | 
| ---------- | ----------------- | ------------------ | ------------- | 
| **价值函数**方法，Value-based  | Q-learning, SARSA | 直接逼近 $$Q^*(s,a)$$ ，再决定动作   | 通过 TD（时间差分）更新 | 
| **策略梯度**方法，Policy-based | REINFORCE, PPO    | 直接优化 $$\pi_\theta(a \| s)$$ 概率分布    | 通过`梯度上升`最大化奖励 |





### 5.1.价值函数（Value-based）

> **核心思想：** 不直接学策略，而是先学“每个状态或动作有多好”，再据此选动作。

这类算法的目标是，逼近`最优` **Q 函数**：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

也就是：每个`状态–动作`对的 **最佳长期回报**。

#### 5.1.1.代表算法

* **Q-learning**
* **Deep Q Network (DQN)**


#### 5.1.2.思路

1. 学一个 **价值函数**（比如  $$Q(s,a)$$ 或 $$V(s)$$ ）。
2. 通过与环境交互、获得奖励，不断更新估计：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
$$

3. 推理阶段，选择动作时，就选 $$Q$$ 值最高的那个动作：

$$
a^* = \arg\max_a Q(s,a)
$$

训练结束的标志：**找到目标价值函数（即 $$Q^*$$）就意味着找到了最优策略**。


> 一旦学到了正确的 $$Q^*$$，在每个状态选 $$Q$$ 最大的动作，就是最优决策、**长期回报最高**。


不过要注意两点：

1. 实际中我们不会“精确找到” $$Q^*$$，而是`逼近`；
2. 收敛标准通常是：Q 值`变化非常小` 或 `策略稳定`不再改进。

训练结束时，收敛结果是：

$$
Q^*(s,a)
$$

$$
\approx r + \gamma \max_{a'} Q^*(s',a')
$$



#### 5.1.3.优点

* 理论扎实，计算高效；
* 不需要显式建模策略。

#### 5.1.4.缺点

* 只能处理**离散动作空间**（连续动作会很难“max”）；
* 不适合直接学习复杂的随机策略。


### 5.2.策略（Policy-based）

> **核心思想：** 直接学习“如何行动”的`策略函数` $$\pi_\theta(a\|s)$$ 概率分布.


#### 5.2.1.代表算法

* **REINFORCE（Policy Gradient）**
* **Proximal Policy Optimization (PPO)**
* **Actor-Critic**
* **PPO / A2C / DDPG / SAC**（现代主流）


#### 5.2.2.思路

1. 策略由参数 $$\theta$$ 决定（通常是一个`神经网络`）。
2. 目标是最大化期望累计奖励：

$$
J(\theta) = \mathbb{E}_{\pi\theta}\Big[\sum_t \gamma^t R_t\Big]
$$

3. 使用梯度上升（Policy Gradient）更新：

$$
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
$$


#### 5.2.3.优点

* 能处理**连续动作空间**；
* 可以`直接优化`期望`收益`；
* 更灵活（可学到随机策略）。

#### 5.2.4.缺点

* 收敛慢、方差大；
* 通常需要配合价值估计（这就引出了 **Actor-Critic**）。



### 5.3.混合路线：Actor–Critic

> 同时学策略（Actor）和价值（Critic），结合两者优点。

结构：

* **Actor**：学策略 $$\pi(a\|s)$$，告诉你“该怎么做”；
* **Critic**：学价值函数 $$V(s)$$ 或 $$Q(s,a)$$，告诉你“做得好不好”。













[NingG]:    http://ningg.github.io  "NingG"










