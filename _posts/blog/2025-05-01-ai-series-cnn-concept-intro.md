---
layout: post
title: AI 系列： CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride)
description: 卷积神经网络，基础知识
published: true
category: AI
---

> 原文：[CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride)](https://zhuanlan.zhihu.com/p/77471866)

![](/images/ai-series/cnn/intro-all.png)


近几年来，在深度学习领域，**卷积神经网络**一度成为大家的`宠儿`，深受大众青睐*（其实就是使用频繁，这没办法啊，效果是真的好，一用就停不下来）*

那卷积神经网络到底是个什么东西啊？我们今天就来看一下。

**卷积神经网络**（convolutional neural network，CNN）是指至少在网络的一层中**使用卷积运算来代替一般的矩阵乘法运算**的神经网络，因此命名为卷积神经网络。那什么是卷积运算啊？接下来我们一起来揭开它神秘的面纱。

## 1.卷积 Convolution

我们以灰度图像为例进行讲解：从一个小小的权重矩阵，也就是`卷积核`（kernel）开始，让它逐步在二维输入数据上“扫描”。卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。

![](/images/ai-series/cnn/demo1.webp)

![](/images/ai-series/cnn/demo2.webp)

深度学习里面所谓的`卷积运算`，其实它被称为 **互相关运算：**

* 1.将图像矩阵中，从左到右，由上到下，取与`滤波器`**同等大小**的一部分，
* 2.每一部分中的值，与滤波器中的值对应`相乘后求和`，
* 3.最后的结果组成一个`矩阵`，其中没有对核进行翻转。

## 2.填充 Padding

前面可以发现，输入图像与`卷积核`进行卷积后的结果中损失了部分值：

* 1.输入图像的边缘，被**修剪掉了**（边缘处只检测了部分像素点，丢失了图片边界处的众多信息）
* 2.这是因为，`边缘上的像素`永远`不会位于卷积核中心`，而卷积核也没法扩展到边缘区域以外。

这个结果我们是不能接受的，有时我们还希望输入和输出的大小应该保持一致。为解决这个问题，可以在进行卷积操作前，对原矩阵进行边界**填充（Padding）**，也就是在**矩阵的边界上填充一些值**，以增加矩阵的大小，通常都用“000”来进行填充的。

![](/images/ai-series/cnn/demo3.webp)

通过填充的方法，当`卷积核`扫描输入数据时，它能延伸到**边缘以外**的`伪像素`，从而使输出和输入size相同。

常用的两种padding：

* **（1）valid padding**：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界
* **（2）same padding**：进行填充，允许卷积核超出原始图像边界，并使得`卷积后`结果的**大小与原来的一致**

## 3.步长 Stride

滑动卷积核时，我们会先从输入的`左上角`开始，每次往`左滑动一列`或者往`下滑动一行`逐一计算输出，我们将**每次滑动的行数和列数**称为 **步长** `Stride`

* 在之前的图片中，Stride=1；
* 在下图中，Stride=2。

![](/images/ai-series/cnn/demo4.webp)


卷积过程中，

* 1. 保持信息：有时需要通过`padding`，来避免信息损失，
* 2. 压缩信息：有时也要在卷积时，通过设置 **步长（Stride）**来**压缩**一部分信息，或者使输出的尺寸小于输入的尺寸。

![](/images/ai-series/cnn/demo5.webp)

**Stride的作用：**是`成倍缩小尺寸`，而这个参数的值就是缩小的具体倍数，比如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。


## 4.典型问题

### 4.1.卷积核 kernel 一般为奇数

> **【卷积核的大小一般为奇数\*奇数】** 1x1，3x3，5x5，7x7都是最常见的。
> 
> **这是为什么呢？**为什么没有偶数\*偶数？

**（1）更容易padding**

在卷积时，我们有时候需要卷积前后的尺寸不变。这时候我们就需要用到padding。

* 假设图像的大小，也就是**被卷积对象**的大小为`n x n`，**卷积核**大小为`k x k`，
* `padding`的幅度设为`(k-1)/2`时，
* 卷积后的输出就为 `(n-k + 2 x ((k-1)/2))/1+1 = n` ，
* 即卷积输出为`n x n`，保证了卷积前后尺寸不变。
* 但是如果k是偶数的话，`(k-1)/2`就不是整数了。

**（2）更容易找到卷积锚点**

在CNN中，进行卷积操作时，一般会以`卷积核模块`的一个位置为`基准`进行滑动，这个基准通常就是`卷积核模块的中心`。

若卷积核为**奇数**，卷积**锚点很好找**，自然就是卷积模块中心，但如果卷积核是偶数，这时候就没有办法确定了，让谁是锚点似乎都不怎么好。

**【卷积的计算公式】**

**输入图片的尺寸：**一般用 `n x n` 表示输入的image大小。

**卷积核的大小：**一般用 `f x f` 表示卷积核的大小。

**填充（Padding）：**一般用 `p x p` 来表示填充大小。

**步长(Stride)：**一般用 `s x s` 来表示步长大小。

**输出图片的尺寸：**一般用 `o x o` 来表示。

如果已知 `n` \ `f` \ `p` \ `s` 可以求得 `o` ，**计算公式如下：

* `o` = `⌊(n + 2p - f) / s ⌋ + 1`


其中 `⌊ ⌋` 是`向下取整`，用于结果不是整数时进行向下取整。


### 4.2.多通道卷积

上述例子都只包含一个输入通道。实际上，大多数输入图像都有 RGB 3个通道。

![](/images/ai-series/cnn/demo6.jpg)

这里就要涉及到“卷积核”和“filter”这两个术语的区别。在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的。但在一般情况下，它们是两个完全不同的概念。**每个“filter”实际上恰好是“卷积核”的一个集合**，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。

**多通道卷积的计算过程：**将矩阵与滤波器对应的每一个通道进行卷积运算，最后相加，形成一个单通道输出，加上偏置项后，我们得到了一个最终的单通道输出。如果存在多个filter，这时我们可以把这些最终的单通道输出组合成一个总输出。

这里我们还需要**注意**一些问题——滤波器的通道数、输出特征图的通道数。

**某一层滤波器的通道数 = 上一层特征图的通道数。**如上图所示，我们输入一张 6×6×3 的RGB图片，那么滤波器（ 3×3×3 ）也要有三个通道。

**某一层输出特征图的通道数 = 当前层滤波器的个数。**如上图所示，当只有一个filter时，输出特征图（ 4×4 ）的通道数为1；当有2个filter时，输出特征图（4×4×2）的通道数为2。










[NingG]:    http://ningg.github.io  "NingG"










