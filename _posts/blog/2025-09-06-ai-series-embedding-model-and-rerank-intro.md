---
layout: post
title: AI 系列：Embedding 与 Rerank
description: Embedding、Rerank 细节描述
published: true
category: AI
---


RAG系统中, 通常会有几个设置:

1.  embedding模型
2.  rerank模型
3.  TopK, TopN

Embedding 比较好理解, 将内容打成向量, 然后可以在查找时通过 夹角大小/余弦相似度找到最接近的向量, 换言之完成了`相似度`​的寻找.

## 1. Embedding 嵌入

**Embedding** 是RAG流程的**第一步**，属于 **“召回（Retrieval）”** 阶段。

*   **核心定义**： Embedding是一种将离散的文本信息（如单词、句子、文档）转换为**稠密、连续的数字向量（Vector）** 的技术。这个向量可以被认为是文本在多维空间中的一个“坐标”，它捕捉了文本的**语义信息**。
    
*   **在RAG中的目标**：**实现高效的语义相似度搜索**。计算机无法直接理解“苹果”和“iPhone”有关联，但通过Embedding模型，这两个词的向量在空间中的位置会非常接近。这使得我们可以通过计算向量间的距离（如余弦相似度）来判断文本间的相关性。
    
*   **工作流程**：

    1.  **离线索引（Indexing）** ：在RAG系统搭建时，我们会预先将知识库中所有的文档块（Chunks）通过一个Embedding模型（如 OpenAI的`text-embedding-ada-002`​ 或开源的 `BGE`​ 系列模型）转换成向量，并存入专门的**向量数据库（Vector Database）** 中。
    2.  **在线查询（Querying）** ：当用户提出问题时，系统使用**同一个Embedding模型**将用户的问题也转换成一个向量。
    3.  **向量检索（Search）** ：系统在向量数据库中，搜索与用户问题向量最“接近”的文档向量，并返回Top-K个（比如K=20）最相似的文档块。

*   **关键特点**：    

    *   **快**：向量检索非常高效，可以在毫秒内从数十亿的向量中找到最近邻。
    *   **广（高召回率）** ：它的目标是“宁可错杀，不能放过”，确保所有可能相关的文档都被包含在初步结果中。
    *   **语义理解**：它超越了传统的关键词匹配，能理解“如何修复我的电脑？”和“我的笔记本无法启动”是相似的问题。

## 2. Rerank 重排

**Rerank** 是RAG流程的**可选但强烈推荐的第二步**，属于 **“精排（Ranking）”** 阶段，发生在Embedding召回之后，LLM生成之前。

*   **核心定义**： Rerank是一种利用更复杂的模型，对Embedding初步检索出的文档列表进行**重新排序**，以提高最相关文档排在最前面的概率的技术。
    
*   **为什么需要Rerank？** Embedding的“快”和“广”是有代价的。它有时会召回一些仅是主题相关但并非答案所在的文档。例如，提问“RAG中的Rerank模型有哪些推荐？”，Embedding可能会召回所有介绍RAG、Embedding和Rerank的文章，但Rerank模型的目标是精准地将那篇**专门对比和推荐Rerank模型**的文章排到第一位。
    
*   **工作流程**：
    
    1.  **输入**：Rerank模型的输入是**用户原始问题**和**Embedding召回的每一个文档块**。它是一个 **(query, document)** 对。
    2.  **计算相关性分数**：Rerank模型（通常是**交叉编码器/Cross-Encoder**）会同时分析问题和文档，输出一个精确的相关性分数（e.g., a score from 0 to 1）。这个过程比Embedding的独立编码要慢得多，因为它需要对每个文档和查询的组合进行深度分析。
    3.  **重新排序**：根据得到的相关性分数，对初步召回的文档列表进行降序排列。
    4.  **筛选**：只保留重排后分数最高的Top-N个（比如N=5）文档，传递给最终的LLM。

*   **关键特点**：
    
    *   **准（高精确率）** ：因为它同时考虑了问题和文档的交互，所以对相关性的判断非常精准。
    *   **慢**：计算成本远高于Embedding。因此，它只适用于处理一个已经经过初筛的小批量文档（比如20-50个），而不是整个知识库。
    *   **降噪**：能有效过滤掉Embedding召回结果中的“噪音”文档，为LLM提供更高质量、更专注的上下文信息，从而提升最终答案的质量。

## 3. TopK/TopN

这两个是参数，而不是模型。它们是用来控制流程中“数量”的“阀门”。在RAG流程中，我们通常会区分使用它们。

*   **TopK (用于召回阶段):**
    
    *   **作用：** 这是**Embedding模型检索后，返回的候选文档数量**。比如，你设置 `K=50`​，意味着Embedding模型会从整个知识库中，找出与查询最相似的 **50** 个文档。
    *   **目的：** `K`​ 值通常设置得比较大。这是为了保证**高召回率**，即确保真正的答案大概率包含在这 `K`​ 个结果中，给后续的Rerank模型提供充足的、高质量的候选材料。
*   **TopN (用于最终选择):**
    
    *   **作用：** 这是**Rerank模型排序后，最终选择送给LLM的文档数量**。比如，你设置 `N=3`​，意味着系统会从Rerank排序后的结果中，挑选出最顶部的 **3** 个文档。
    *   **目的：** `N`​ 值通常设置得比较小。这是因为LLM的上下文窗口（Context Window）是有限的，不能无限输入信息。选择最相关、信息最浓缩的 `N`​ 个文档，可以获得最佳的生成效果，同时避免无关信息干扰LLM的判断。
*   **区别 (Difference):**
    
    *   **应用阶段不同：** `TopK`​ 用于召回阶段的输出，是Rerank模型的**输入**。`TopN`​ 用于精排阶段的输出，是LLM的**输入**。
    *   **数量大小不同：** 通常 `K`​ 远大于 `N`​ (例如: K=50, N=3)。
    *   **目标不同：** `TopK`​ 关注“别漏掉”，`TopN`​ 关注“只给最好的”。

## 4. Embedding模型的评估

我们该怎么判断一个embedding是好还是坏呢? 有什么典型的评判标准呢?

以Qwen3新发布的[embedding模型博客](https://qwenlm.github.io/zh/blog/qwen3-embedding/)作为起点, 让我们继续看看.

Embedding模型的Benchmark主要围绕一个核心问题：

* 这个模型`生成的向量`，能不能在`各种任务`中**准确地衡量**出文本之间的`语义关系`？

为此，业界建立了一套标准化的评测集和评测方法，其中最著名和最权威的就是 **MTEB (Massive Text Embedding Benchmark)** 。你在Qwen的博客中看到的`MTEB-R`​, `CMTEB-R`​等，都是基于MTEB体系的。

### 4.1. 核心评测任务分类

Embedding模型的评测不是单一维度的，而是涵盖了多种任务，以全面考察其能力。MTEB将这些任务分成了几个大类：

*   **检索 (Retrieval):** 这是最核心、最常见的任务，特别是在RAG场景下。
    
    *   **做法：** 给定一个查询（Query），模型需要在庞大的文档库中找到最相关的文档。
    *   **评测指标：** 通常使用 **nDCG@k** (归一化折损累计增益) 或 **MAP@k** (平均精度均值) 等指标。简单来说，就是看模型找出的前k个结果，是不是用户真正想要的，并且想要的排得越靠前，得分越高。
    *   **例子：** Qwen的文章中提到的 `MTEB-R`​ (英文检索), `CMTEB-R`​ (中文检索), `MMTEB-R`​ (多语言检索) 和 `MTEB-Code`​ (代码检索) 都属于这一类。

*   **重排 (Reranking):** 这个任务专门用来评测Rerank模型，但其原理与Embedding模型评测相通。
    
    *   **做法：** 给定一个查询和一组候选文档（通常是检索阶段的TopK结果），模型需要对这些文档进行精准排序。
    *   **评测指标：** 同样使用nDCG、MAP等指标，但衡量的是对一个小集合的排序能力。
    *   **例子：** Qwen的文章中明确区分了`Embedding`​模型和`Reranker`​模型，并分别给出了评测结果。

*   **分类 (Classification):**
    
    *   **做法：** 将文本向量化后，训练一个简单的分类器（如逻辑回归），看这个向量能不能很好地支持对文本进行分类（如情感分析、主题分类）。
    *   **评测指标：** 准确率 (Accuracy) 或 F1分数。

*   **聚类 (Clustering):**
    
    *   **做法：** 将一组文本向量化后，进行聚类算法，看语义相近的文本是否能被分到同一个簇中。
    *   **评测指标：** V-measure 等指标。

*   **语义文本相似度 (Semantic Textual Similarity, STS):**
    
    *   **做法：** 给定两个句子，模型输出一个相似度分数（通常是计算两个句子向量的余弦相似度）。将这个分数与人类标注的“黄金标准”分数进行比较。
    *   **评测指标：** 皮尔逊（Pearson）或斯皮尔曼（Spearman）相关系数，看机器打分和人类打分的相关性有多强。

### 4.2. 标准化的评测数据集

为了公平比较，Benchmark必须在公开、标准的数据集上进行。MTEB整合了来自不同任务和语言的大量数据集。

*   **多语言能力：** Qwen的评测中特别强调了 `MMTEB-R`​ (多语言) 和 `CMTEB`​ (中文)，这表明现代的Embedding模型非常看重跨语言和多语言能力。
*   **领域适应性：** 除了通用文本，还会评测在特定领域（如代码、金融、医疗）的表现，例如 `MTEB-Code`​ 就是针对代码检索的。


> 独立的 Embedding model、Rerank Model 评估。
> 
> 
> 1.**Embedding Model 的独立评估**
> 
> **目标**：检索时 embedding 的“语义表征能力”。
> 
> 常见指标：
> 
> * **Retrieval Quality（检索质量）**
> 
>   * *Recall\@k*：Top-k 检索结果中是否包含正确答案。
>   * *Precision\@k*：Top-k 中相关文档占比。
>   * *MRR（Mean Reciprocal Rank）*：正确文档出现的倒数排名均值。
>   * *nDCG（Normalized Discounted Cumulative Gain）*：考虑排序位置的加权相关度。
> 
> * **Embedding 表征评估**
> 
>   * *Clustering Purity / NMI / ARI*：聚类效果。
>   * *STS（Semantic Textual Similarity）*：与人工打分的句子语义相似度对比。
>   * *Domain Adaptation Check*：在目标领域是否维持语义区分度。
> 
> 2.**Rerank Model 的独立评估**
> 
> **目标**：在候选文档集合中，模型是否能把“更相关”的排在前面。
> 
> 常见指标（多用于信息检索 IR 领域）：
> 
> * *MAP（Mean Average Precision）*：多个 query 的平均准确率。
> * *MRR（Mean Reciprocal Rank）*：关注第一个相关文档的排名。
> * *nDCG（Normalized Discounted Cumulative Gain）\@k*：加权排序质量，越相关的文档排得越靠前得分越高。
> * *Hit Rate\@k*：前 k 个结果里是否有相关文档。
> * *Pairwise Accuracy*：`成对`比较文档时，模型是否正确判断哪个更相关。
> 
> **总结**：
> 
> * **Embedding model** → Recall\@k, MRR, nDCG, STS
> * **Rerank model** → MAP, nDCG, Pairwise Accuracy
> 
> 这样，RAG 效果可以从 **检索-排序-生成** 三个环节独立衡量，也能整体衡量。



关联资料

[RAG两大核心利器: M3E-embedding和bge-rerank](https://www.cnblogs.com/theseventhson/p/18273943)




































[NingG]:    http://ningg.github.io  "NingG"










