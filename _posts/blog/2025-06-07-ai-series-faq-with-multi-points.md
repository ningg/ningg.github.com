---
layout: post
title: AI 领域，常见问题 FAQ
description: 零散原理汇总
published: true
category: AI
---


0.术语汇总


术语全称：包含要解决的问题

* VSM：向量空间模型（Vector Space Model, VSM），就是词嵌入
* Word2Vec：语言模型（单层神经网络），实现预训练的词嵌入，Word2Vec的核心思想：利用词在文本中的`上下文信息`来捕捉词之间的`语义`关系，从而使得语义相似或相关的词在向量空间中距离较近。
* BERT：根据句子，动态调整 词嵌入。Bidirectional Encoder Representations from Transformers， Encoder-Only 模型，自然语言理解（Natural Language Understanding-NLU）任务。
* ELMo：Embeddings from Language Models，实现了一词多义、静态词向量到动态词向量的跨越式转变。首先在大型语料库上训练语言模型，得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量，ELMo首次将预训练思想引入到词向量的生成中，使用双向LSTM结构，能够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示。
* LSTM：Long Short-Term Memory，长短时记忆网络，
* MLM：掩码语言模型（Masked Language Model，MLM）
* SOTA：最优性能（State Of The Art，SOTA）
* NSP（Next Sentence Prediction，下一句预测）：token 之间，有关系；句子之间，也有关系。关系/练习就是信息。
* T5（Text-To-Text Transfer Transformer）是由 Google 提出的一种预训练语言模型，通过将所有 NLP 任务统一表示为文本到文本的转换问题，大大简化了模型设计和任务处理。
* GPT：Generative Pre-Training Language Model，是由 OpenAI 团队于 2018年发布的预训练语言模型。虽然学界普遍认可 BERT 作为预训练语言模型时代的代表，但首先明确提出预训练-微调思想的模型其实是 GPT。GPT 提出了通用预训练的概念，也就是在海量无监督语料上预训练，进而在每个特定任务上进行微调，从而实现这些任务的巨大收益。
* CLM：因果语言模型，Casual Language Model，下简称 CLM，Decoder-Only 的模型结构往往更适合于文本生成任务，因此，Decoder-Only 模型往往选择了最传统也最直接的预训练任务——因果语言模型。
* PLM：`预训练`语言模型（Pre-trained Language Model，PLM）
* AGI：通用人工智能（Artificial General Intelligence，AGI）
* LLM，即 Large Language Model，中文名为大语言模型或大型语言模型，是一种相较传统语言模型参数量更多、在更大规模语料上进行预训练的语言模型。
* 涌现能力（Emergent Abilities）：涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。可以类比到物理学中的相变现象，涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变。
* 上下文学习（In-context Learning）：上下文学习是指允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。
* RoPE：LLM 大部分采用了旋转位置编码（Rotary Positional Encoding，RoPE）（或者同样具有外推能力的 AliBi）作为位置编码，具有一定的长度外推能力，也就是在推理时能够处理显著长于训练长度的文本。
* `监督微调`（Supervised Fine-Tuning，`SFT`），SFT 的主要目标是让模型从多种类型、多种风格的指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。
* `强化学习与人类反馈`（Reinforcement Learning with Human Feedback，`RLHF`）
* PPO，Proximal Policy Optimization，近端策略优化算法
* DPO（Direct Preference Optimization，直接偏好优化）：






1.嵌入 Embedding

在机器学习和自然语言处理（NLP）中，嵌入（Embedding） 是一种将离散型数据（如词语、类别、符号）转换为连续型向量表示的技术。

将符号映射到连续向量空间，使机器能够数学化地理解语义。

其本质是，构建一个从`高维稀疏空间`到`低维稠密空间`的**映射**，旨在捕捉数据背后的语义或结构关系。



2.层归一化

层归一化，也就是 `Layer Norm`，是深度学习中经典的归一化操作。

归一化核心是，为了让不同层输入的`取值范围`或者`分布`能够比较一致。


由于深度神经网络中每一层的输入都是上一层的输出，因此多层传递下，对网络中较高的层，之前的所有神经层的参数变化会导致其输入的分布发生较大的改变。
也就是说，随着神经网络参数的更新，各层的输出分布是不相同的，且差异会随着网络深度的增大而增大。但是，需要预测的条件分布始终是相同的，从而也就造成了预测的误差。

在深度神经网络中，往往需要`归一化`操作，将每一层的`输入`都归一化成`标准正态分布`。


在深度神经网络中更常用、效果更好的层归一化（Layer Norm）。相较于 Batch Norm 在`每一层统计`所有样本的均值和方差，Layer Norm 在`每个样本`上计算其`所有层`的均值和方差，从而使每个样本的分布达到稳定。Layer Norm 的归一化方式其实和 Batch Norm 是完全一样的，只是统计统计量的维度不同。



3.残差连接

残差连接， Residual Connection。

由于 Transformer 模型结构较复杂、层数较深，​为了避免模型退化，Transformer 采用了残差连接的思想来连接每一个子层。残差连接，即下一层的输入不仅是上一层的输出，还包括上一层的输入。

残差连接允许最底层信息直接传到最高层，让高层专注于残差的学习。




4.前馈神经网络

前馈神经网络（Feed Forward Neural Network，下简称 FFN）：`每一层`的`神经元`都和`上下两层`的每一个神经元`完全连接`的网络结构。


每一个 `编码层` Encoder Layer ，都包含一个上文讲的`注意力机制`和一个`前馈神经网络`。

前馈神经网络，有什么作用？

* 用于处理`特征`的`非线性变换`




5.线性层 Linear


有什么作用？


6.激活函数


激活函数，只是为了，结果分布变成标准正态分布？


7.dropout 函数


什么作用？


8.Decoder 解码

先搭建 Decoder Layer，再将 N 个 Decoder Layer 组装为 Decoder。

但是和 Encoder 不同的是，Decoder 由两个注意力层和一个前馈神经网络组成。

1. 第一个注意力层是一个掩码自注意力层，即使用 Mask 的注意力计算，保证每一个 token 只能使用该 token 之前的注意力分数；
2. 第二个注意力层是一个多头注意力层，该层将使用第一个注意力层的输出作为 query，使用 Encoder 的输出作为 key 和 value，来计算注意力分数


9.预训练+微调范式


在 2018年，ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。而 BERT 也采用了该范式，并通过将模型架构调整为 Transformer，引入更适合文本理解、能捕捉深层双向语义关系的预训练任务 MLM，将预训练-微调范式推向了高潮。

预训练-微调范式的核心优势在于，通过将预训练和微调分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上，只要微调的成本较低，即使预训练成本是之前的数倍甚至数十倍，模型仍然有更大的应用价值。

GPT-2 的另一个重大突破是以 `zero-shot`（零样本学习）为主要目标，也就是不对模型进行微调，`直接要求模型`解决任务。例如，在传统的预训练-微调范式中，我们要解决一个问题，一般需要收集几百上千的训练样本，在这些训练样本上微调预训练语言模型来实现该问题的解决。而 zero-shot 则强调不使用任何训练样本，直接通过向预训练语言模型描述问题来去解决该问题。zero-shot 的思路自然是比预训练-微调范式更进一步、更高效的自然语言范式，但是在 GPT-2 的时代，模型能力还不足够支撑较好的 zero-shot 效果，在大模型时代，zero-shot 及其延伸出的 `few-shot`（少样本学习）才开始逐渐成为主流。

通过给模型提供少量示例，模型可以取得远好于 zero-shot 的良好表现。`few-shot` 也被称为`上下文学习`（**In-context Learning**），即让模型从提供的上下文中的示例里学习问题的解决方法。GPT-3 在 few-shot 上展现的强大能力，为 NLP 的突破带来了重要进展。如果对于绝大部分任务都可以通过人为构造 3~5个示例就能让模型解决，其效率将远高于传统的预训练-微调范式，意味着 NLP 的进一步落地应用成为可能——而这，也正是 LLM 的核心优势。

在 GPT 系列模型的基础上，通过引入`预训练`-`指令微调`-`人类反馈强化学习`的三阶段训练，OpenAI 发布了跨时代的 ChatGPT，引发了大模型的热潮。


10.T5

学习 Encoder-Decoder 结构的模型，主要介绍 T5 的模型架构和预训练任务，以及 T5 模型首次提出的 NLP 大一统思想。

T5（Text-To-Text Transfer Transformer）是由 Google 提出的一种预训练语言模型，通过将所有 NLP 任务统一表示为文本到文本的转换问题，大大简化了模型设计和任务处理。T5 基于 Transformer 架构，包含编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。


11.CLM 因果语言模型

因果语言模型，Casual Language Model，CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，通过不断重复该过程来实现目标文本序列的生成。也就是说，CLM 是一个经典的补全形式。


12.GLM

GLM 的核心创新点主要在于其提出的 GLM（General Language Model，通用语言模型）任务，这也是 GLM 的名字由来。GLM 是一种结合了自编码思想和自回归思想的预训练方法。所谓自编码思想，其实也就是 MLM 的任务学习思路，在输入文本中随机删除连续的 tokens，要求模型学习被删除的 tokens；所谓自回归思想，其实就是传统的 CLM 任务学习思路，也就是要求模型按顺序重建连续 tokens。

GLM 系列模型是由智谱开发的主流中文 LLM 之一，包括 ChatGLM1、2、3及 GLM-4 系列模型，覆盖了指令理解、代码生成等多种应用场景，曾在多种中文评估集上达到 SOTA 性能。


在整体模型结构上，GLM 和 GPT 大致类似，均是 Decoder-Only 的结构，仅有三点细微差异：

1.使用 Post Norm 而非 Pre Norm。Post Norm 是指在进行残差连接计算时，先完成残差计算，再进行 LayerNorm 计算；而类似于 GPT、LLaMA 等模型都使用了 Pre Norm，也就是先进行 LayerNorm 计算，再进行残差的计算。相对而言，Post Norm 由于在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；Pre Norm相对于因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模型的梯度爆炸或者梯度消失。因此，对于更大体量的模型来说，一般认为 Pre Norm 效果会更好。但 GLM 论文提出，使用 Post Norm 可以避免 LLM 的数值错误（虽然主流 LLM 仍然使用了 Pre Norm）；

2.使用单个线性层实现最终 token 的预测，而不是使用 MLP；这样的结构更加简单也更加鲁棒，即减少了最终输出的参数量，将更大的参数量放在了模型本身；

3.激活函数从 ReLU 换成了 GeLUS。ReLU 是传统的激活函数，其核心计算逻辑为去除小于 0的传播，保留大于 0的传播；GeLUS 核心是对接近于 0的正向传播，做了一个非线性映射，保证了激活函数后的非线性输出，具有一定的连续性。



13.LLM

一般来说，LLM 指包含数百亿（或更多）参数的语言模型，它们往往在数 T token 语料上通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM 一般覆盖了从十亿参数（如 Qwen-1.5B）到千亿参数（如 Grok-314B）的所有大型语言模型。只要模型展现出涌现能力，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。


一般认为，`GPT-3`（1750亿参数）是 LLM 的**开端**，基于 GPT-3 通过 `预训练`（Pre-training）、`监督微调`（Supervised Fine-Tuning，`SFT`）、`强化学习与人类反馈`（Reinforcement Learning with Human Feedback，`RLHF`）三阶段训练得到的 ChatGPT 更是主导了 LLM 时代的到来。

LLM 的能力：

* 1.涌现能力（Emergent Abilities）
* 2.上下文学习（In-context Learning）
* 3.指令遵循（Instruction Following）
* 4.逐步推理（Step by Step Reasoning）

此外，LLM 有几个方向，待进一步优化：

* 1.多语言支持
* 2.拓展多模态
* 3.长文本处理
* 4.挥之不去的幻觉：削弱幻觉的一些方法，如 Prompt 里进行限制、通过 RAG（检索增强生成）来指导生成等，但都还只能一定程度减弱幻觉而无法彻底根除。



14.上下文学习（In-context Learning）

上下文学习（In-context Learning）：是指允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。

上下文学习能力也正在引发 NLP 研究范式的变革。在传统 PLM 时代，解决 NLP 下游任务的一般范式是预训练-微调，即选用一个合适的预训练模型，针对自己的下游任务准备有监督数据来进行微调。而通过使用具备上下文学习能力的 LLM，一般范式开始向 Prompt Engineering 也就是调整 Prompt 来激发 LLM 的能力转变。例如，目前绝大部分 NLP 任务，通过调整 Prompt 或提供 1~5 个自然语言示例，就可以令 GPT-4 达到超过传统 PLM 微调的效果。



15.指令遵循（Instruction Following）

通过使用自然语言描述的多任务数据进行微调，也就是所谓的 `指令微调` ，LLM 被证明在同样使用指令形式化描述的`未见过的任务`上表现良好。也就是说，经过指令微调的 LLM 能够理解并遵循未见过的指令，并根据任务指令执行任务，而无需事先见过具体示例，这展示了其强大的泛化能力。



16.逐步推理（Step by Step Reasoning）

LLM 通过采用思维链（Chain-of-Thought，CoT）推理策略，可以利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。

逐步推理能力意味着 LLM 可以处理复杂逻辑任务，也就是说可以解决日常生活中需要逻辑判断的绝大部分问题，从而向“可靠的”智能助理迈出了坚实的一步。



17.RLHF 人类反馈强化学习

RLHF，全称是 Reinforcement Learning from Human Feedback，即人类反馈强化学习

RLHF 就类似于 LLM 作为一个学生，不断做作业来去提升自己解题能力的过程。

如果把 LLM 看作一个能力强大的学生，

* 1.Pretrain 是将所有基础的知识教给他，
* 2.SFT 是教他怎么去读题、怎么去解题，
* 3.那么 RLHF 就类似于真正的练习。

LLM 会不断根据 Pretrain 学到的基础知识和 SFT 学到的解题能力去解答练习，然后人类作为老师批改 LLM 的练习，来让 LLM 反思错误的解题方式，不断强化正确的解题方式。


RLHF 分为两个步骤：训练 RM 和 PPO 训练。


18.RM

RM，Reward Model，即奖励模型。RM 是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM 的每一个回复，RM 会进行打分，这个打分反映了生成回复符合人类偏好的程度。然后 LLM 会根据强化学习的原理，基于 RM 的打分来进行优化训练。

RM 本质上是一个文本分类模型，对于一个文本输出一个标量奖励，和文本分类任务中的隐藏层输出非常类似。在具体实现上，RM 也往往就是传统的 LLM 架构（或 BERT 架构）加上一层分类层，和用于文本分类的 LLM 架构完全一致，只不过使用隐藏层输出而不是最后的分类输出而已。

19.PPO 近端策略优化算法

在完成 RM 训练之后，就可以使用 PPO 算法来进行强化学习训练。`PPO`，Proximal Policy Optimization，`近端策略优化算法`，是一种经典的 RL 算法。事实上，强化学习训练时也可以使用其他的强化学习算法，但目前 PPO 算法因为成熟、成本较低，还是最适合 RLHF 的算法。

在具体 PPO 训练过程中，会存在四个模型。如图4.5所示，`两个 LLM` 和`两个 RM`。两个 LLM 分别是进行微调、参数更新的 actor model 和不进行参数更新的 ref model，均是从 SFT 之后的 LLM 初始化的。两个 RM 分别是进行参数更新的 critic model 和不进行参数更新的 reward model，均是从上一步训练的 RM 初始化的。

因为要使用到四个模型，显存占用会数倍于 SFT。例如，如果我们 RM 和 LLM 都是用 7B 的体量，PPO 过程中大概需要 240G（4张 80G A100，每张卡占用 60G）显存来进行模型加载。那么，为什么我们需要足足四个模型呢？Actor Model 和 Critic Model 较为容易理解，而之所以我们还需要保持原参数不更新的 Ref Model 和 Reward Model，是为了限制模型的更新不要过于偏离原模型以至于丢失了 Pretrain 和 SFT 赋予的能力。


如此大的资源占用和复杂的训练过程，使 RLHF 成为一个门槛非常高的阶段。也有学者从监督学习的思路出发，提出了 DPO（Direct Preference Optimization，直接偏好优化），可以低门槛平替 RLHF。

20.DPO 直接偏好优化

 `DPO`（Direct Preference Optimization，`直接偏好优化`），可以低门槛平替 RLHF。
 
 DPO 的核心思路是，将 RLHF 的强化学习问题转化为监督学习来直接学习人类偏好。DPO 通过使用`奖励函数`和`最优策略`间的**映射**，展示了**约束奖励最大化问题**完全可以通过`单阶段策略训练`进行优化，也就是说，通过学习 DPO 所提出的优化目标，可以直接学习人类偏好，而无需再训练 RM 以及进行强化学习。
 
 由于直接使用监督学习进行训练，DPO 只需要两个 LLM 即可完成训练，且训练过程相较 PPO 简单很多，是 RLHF 更简单易用的平替版本。DPO 所提出的优化目标为什么能够直接学习人类偏好，作者通过一系列的数学推导完成了证明，感兴趣的读者可以下来进一步阅读












## 关联资料

* [Happy-LLM](https://datawhalechina.github.io/happy-llm)
* [https://huggingface.co/learn](https://huggingface.co/learn)


































[NingG]:    http://ningg.github.io  "NingG"










